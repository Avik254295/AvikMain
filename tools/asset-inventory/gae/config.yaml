# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#            http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

env_variables:

  # Where the export is written to. A GCS path like gs://my-bucket-name/asset_export
  GCS_DESTINATION: <ENTER-VALUE>

  # Organization number (organizations/123) or project id (projects/id) or number (projects/123)
  EXPORT_PARENT: <ENTER-VALUE>

  # BigQuery dataset to load to.
  IMPORT_DATASET: <ENTER-VALUE>

  # Location to write intermediary data to load from from. A GCS path like gs://my-bucket-name/asset_export/stage
  IMPORT_STAGE: <ENTER-VALUE>

  # Project id to run the dataflow job in when using the template or dataflow runner.
  IMPORT_DATAFLOW_PROJECT: <ENTER_VALUE>

  # Export resources, iam_policy or both.
  EXPORT_CONTENT_TYPES: RESOURCE, IAM_POLICY

  # Comma seprated list of asset types to export such as google.compute.Firewall,google.compute.HealthCheck default is '*' for everything.
  EXPORT_ASSET_TYPES: "*"

  # Can be direct, dataflow or template (however App Engine Standard doesn't support the direct or dataflow runner).
  IMPORT_PIPELINE_RUNNER: template

  # Region to run the dataflow pipeline in.
  IMPORT_TEMPLATE_REGION: us-central1

  # If running a template. This is the template location.
  IMPORT_TEMPLATE_LOCATION: gs://professional-services-tools-asset-inventory/latest/import_pipeline

  # How to group exported resources into Bigquery tables. Either ASSET_TYPE_VERSION or ASSET_TYPE
  IMPORT_GROUP_BY: ASSET_TYPE

  # If tables should be ovewritten (WRITE_EMPTY) or appended (WRITE_APPEND) to.
  IMPORT_WRITE_DISPOSITION: WRITE_APPEND

  # If we are running on App Engine and only want to be invoked by cron tasks for security reasons.
  RESTRICT_TO_CRON_TASKS: "true"

  # When using direct or dataflow running these are the  other arguments to supply to import pipeline.
  IMPORT_PIPELINE_ARGUMENTS: --max_num_workers=10

  # When using the template runner. Supply values as a json representation of RuntimeEnvironment:  https://cloud.google.com/dataflow/docs/reference/rest/v1b3/RuntimeEnvironment
  IMPORT_PIPELINE_RUNTIME_ENVIRONMENT: >
    {
      "maxWorkers": 10
    }

  # For the brave why wish to try running the python direct runner on python3.
  BEAM_EXPERIMENTAL_PY3: "true"
