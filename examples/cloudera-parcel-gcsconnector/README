# Install Google Cloud Storage connector on CDH using Cloudera parcel
The goal of this script is to create Cloudera parcel with GCS connector jar and deploy on a Cloudera managed cluster.

## Prerequisites
### 1. 
### 1.Get the service account JSON key from your GCP account.
4. Download create_parcel.sh file from here(add link) [* if not cloned already]


Installing
Bundle the json key, Cloud Storage Connector jar into a CDH parcel using the build_parcel.sh script.

Instructions to run the script:
Place the following three files in the same folder [Make sure that there are no other files apart from the one’s to be bundled into the parcel]
{service_account_key}.json
Cloud_storage_connector.jar
create_parcel.sh script
Run the script in below format
bash create_parcel.sh -f parcel_name -v version -o operating_system -d 

Where,
-f flag is for parcel_name : is name of the parcel in a single string format without any spaces or special characters.
-v flag is for version : is the version of the parcel in the format x.x.x (ex: 1.0.0)
-o flag is for operating_system : is the name of the operating system to be chosen from this list (rhel5, rhel6, rhel7, suse11, ubuntu10, ubuntu12, ubuntu14, debian6, debian7)
-d flag is to be used if you want to deploy the parcel to the cloudera manager parcel repo folder, this flag is optional and if not provided then the parcel file will be created in the same directory where create_parcel.sh is script run.


Example

For the name of parcel as “gcsconnector”, version as “1.0.0”, and os type rhel6, run the below command.

bash create_parcel.sh -f pcscon -v 1.0.0 -o el6 -d

Deployment
Once the script runs successfully go the Cloudera Manager home page > Hosts > Parcels 
Click on check parcels on the top right corner > wait for the new parcel to populate in the list of parcels > Distribute and activate the parcel.
Our parcel should populate in the list of parcels. > Click on distribute.
Once distributed, we can activate the parcel which would send it to all the CM connected hosts.
Once activated successfully, restart all the stale services.

Check below path for logs:
/var/log/build_script.log

Using services with GCS connector
HDFS service
Go to HDFS service > configurations > type core-site.xml in the search bar > add the following properties in the Cluster-wide Advanced Configuration Snippet (Safety Valve) for core-site.xml

Name : google.cloud.auth.service.account.enable
Value : true

Name : google.cloud.auth.service.account.json.keyfile
Value : {full path to JSON keyfile downloaded for service account}
Example : /opt/cloudera/parcels/gcsconnector/lib/hadoop/lib/key.json

Name : fs.gs.project.id
Value : {GCP project ID}

Name : fs.gs.impl
Value : com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem



Save configurations > Restart required services.

Export Java and hadoop classpath pointing to the gcsconnector jar.

export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/cloudera/parcels/gcsconnector-1.0.0/lib/hadoop/lib/gcs-connector-latest-hadoop2.jar

Run hdfs ls command to validate settings:

hdfs dfs -ls gs://bucket_name

Spark Service


From Cloudera Manager home page go to spark > configuration > Spark Service Advanced Configuration Snippet (Safety Valve) for spark-conf/spark-env.sh

Add configuration according to the gcs connector jar path.


val src=sqlContext.read.json("gs://dataproc-324dd107-1b93-4417-a5c1-c85b719e15e9-us-central1/driver_data/some_sample.json")

Hive Service

From Cloudera Manager home page go to Hive service > configuration > Hive Auxiliary JARs Directory > (provide path to gcs-connector.jar file)


/opt/cloudera/parcels/gcsconnector/lib/hadoop/lib/


Validate if jar is being accepted:

Navigate to beeline and connect to HiveServer2.
