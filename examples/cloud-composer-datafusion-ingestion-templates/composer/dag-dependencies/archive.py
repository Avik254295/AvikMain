#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright 2021 Google LLC All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Module for archival of processed and temp files
function archive_file: copies the files to target location and
deletes the file from original location
function archive: Main archival function to be called by the DAG
invokes the archive_file function for data files and temp files
"""
import os
import logging
from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook
from google.cloud.exceptions import NotFound

def archive_file(
    gcs_hook,
    system,
    orchestrator_dag_run_id,
    source_bucket,
    source_name,
    archival_bucket):
    """
    Copies a file from source bucket to archival bucket with a new name
    Deletes the file from source location
    :param system(string):
        name of the source system of which files are being archived
    :param orchestrator_dag_run_id(string):
        dag_run_id of the orchestrator dag which called the worker DAG
        calling this function
    :param source_bucket(string):
        name of source bucket from which the file should be archived
    :param source_name(string):
        name of source file to be archived
    :param archival_bucket(string):
        name of bucket to which the file should be archived
    """
    basename  = os.path.basename(source_name)
    target_name = "%s/%s/%s" % (system, orchestrator_dag_run_id, basename)
    try:
        gcs_hook.copy(source_bucket, source_name, archival_bucket, target_name)
        gcs_hook.delete(source_bucket, source_name)
        logging.debug(
            f"File {source_name} in bucket {source_bucket} "
            f"moved to bucket {archival_bucket} "
            f"as {target_name}")
    except NotFound:
        logging.debug(
            f"{source_name} not found, might "
            f"already be archived by another child DAG.")

def archive(dag_run_id_det,**context):
    """Main archival method to be called by the DAG.
    :param dag_run_id_det(string):
        run id of the DAG generated by Airflow
    """

    composer_bucket =  context["ti"].xcom_pull(
        key="composer_bucket")
    data_bucket = context["ti"].xcom_pull(
        key="data_bucket")
    archival_bucket = context["ti"].xcom_pull(
        key="archival_bucket")
    system = context["ti"].xcom_pull(
        key="system")
    source_type = context["ti"].xcom_pull(
        key="source_type")
    logging.debug(
        f"printing xcom: data_bucket:{data_bucket}, "
        f"archival_bucket: {archival_bucket}, "
        f"composer_bucket: {composer_bucket}, "
        f"system: {system}, "
        f"dag_run_id: {dag_run_id_det}"
    )

    gcs_hook = GoogleCloudStorageHook("google_cloud_storage_default")
    orchestrator_dag_run_id = context["ti"].xcom_pull(
        key="orchestrator_dag_run_id")

    # Archive source data files
    if source_type in ["gcs"]:
        ##  Retrieve list of source data files to be archived
        source_list = context["ti"].xcom_pull(key="source_list")

        ## Move file from source to archival bucket
        for source_file_dict in source_list:
            source_name = source_file_dict["src_file_name"]
            archive_file(
                gcs_hook,
                system,
                orchestrator_dag_run_id,
                data_bucket,
                source_name,
                archival_bucket)

    # Archive param and temp files
    cdf_arg_abs_filename = context["ti"].xcom_pull(
        key="cdf_arg_abs_filename")

    map_src_dl_target_abs_filename = context["ti"].xcom_pull(
        key="map_src_dl_target_abs_filename")

    src_list_abs_filename = context["ti"].xcom_pull(
        key="src_list_abs_filename")

    ## Move temp files from source to archival bucket
    archive_file(
        gcs_hook,
        system,
        orchestrator_dag_run_id,
        composer_bucket,
        cdf_arg_abs_filename,
        archival_bucket)

    archive_file(
        gcs_hook,
        system,
        orchestrator_dag_run_id,
        composer_bucket,
        map_src_dl_target_abs_filename,
        archival_bucket)

    archive_file(
        gcs_hook,
        system,
        orchestrator_dag_run_id,
        composer_bucket,
        src_list_abs_filename,
        archival_bucket)