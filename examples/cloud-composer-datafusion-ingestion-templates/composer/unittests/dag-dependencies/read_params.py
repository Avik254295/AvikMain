#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright 2021 Google LLC All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Mocked module for testing
This module reads the parameters from all three parameter files
environment parameter, dag paramter and task parameter
"""
import json
import logging
import os
# from airflow.models import Variable
# from airflow import AirflowException
import sys
from pathlib import Path
import importlib
# import ValueError

sys.path.append('/Users/nehajo/projects/datalake-templates/unittests/dag-dependencies')
# models = importlib.import_module("composer.unittests.dag-dependencies.models")
from models import Variable
# import models
print("current working directory (read_params.py):"+ os.getcwd())
class ReadParameters:
    """ReadParameters class reads parameters from the 3 parameter files
    function read_parameters:
        Reads parameters from param files
        and assigns the values to the class variables.
        This is the main function
        to be called whenever parameters need to be read.
    """
    env_param_file=""
    gcp_project_id = ""
    location = ""
    env_name = ""
    audit_dataset = ""
    source_regex = ""
    target_dataset = ""
    data_bucket = ""
    archival_bucket = ""
    composer_bucket = ""
    cdf_namespace = ""
    cdf_instance_name = ""
    param_file_path = ""
    source_type = ""
    dag_load_type = ""
    sftptogcs_dest_path = ""
    gcs_src_file_path = ""
    gcs_archival_path = ""
    metadata_path =""
    sftptogcs_dest_path =""
    dbconnection = ""
    port = ""
    username = ""
    input_database = ""
    insert_audit_cols = ""
    cdf_pipeline_timeout = ""
    update_metadata = ""
    refresh_authorized_views=""

    task_params=[]

    audit_ins_template = ""
    audit_upd_template = ""

    default_delimiter = ","
    delimiter = ""

    def get(self, param):
        return getattr(self, param, None)

    def read_parameters(self,system,dag_param_file_nm,task_param_file_nm):
        """
        Reads parameters from param files
        and assigns the values to the class variables.
        This is the main function
        to be called whenever parameters need to be read.
        :param system(string):
            run id of the DAG generated by Airflow
        :param dag_param_file_nm(string):
            name of dag param file
        :param task_param_file_nm(string):
            name of task param file
        """
        logging.debug("Starting to initialize paramters from files")
        logging.debug(f"gcp project initialized to {self.gcp_project_id}")

        ###################################
        # Read airflow variable for env parameter file
        try:
            self.env_param_file = Variable.get("env_param")
        except KeyError:
            logging.error(
                "Missing airflow variable env_param "
                "for environment parameter file")
            raise

        ###################################
        # Get parameters from parameter files
        ###################################

        # Read env parameter file and set parameter values
        logging.debug(
            f"Reading environment parameter file "
            f"from : {self.env_param_file}")
        try:
            with open(self.env_param_file) as env_param_json:
                env_param = json.load(env_param_json)

                self.gcp_project_id = env_param["gcp_project_id"]
                self.location = env_param["location"]
                self.env_name = env_param["env_name"]

                if self.env_name == "dev" or self.env_name == "test":
                    self.audit_dataset = \
                        env_param["audit_dataset"]+"_"+self.env_name
                else:
                    self.audit_dataset = env_param["audit_dataset"]

                self.param_file_path = env_param["param_file_path"]

                dag_param_file_path = self.param_file_path \
                    + system+"/"+ dag_param_file_nm
                task_param_file_path= self.param_file_path \
                    + system+"/"+ task_param_file_nm

                logging.debug(f"dag_param_file_path: {dag_param_file_path}")
                logging.debug(f"task_param_file_path: {task_param_file_path}")

                self.data_bucket = env_param["data_bucket"]
                self.archival_bucket = env_param["archival_bucket"]
                self.composer_bucket = env_param["composer_bucket"]
                self.cdf_instance_name = env_param["cdf_instance_name"]
                self.cdf_namespace = env_param["cdf_namespace"]
        except IOError:
            logging.error(f"Error opening env_param_file {self.env_param_file}")
            raise

        # Read dag parameter file and set parameter values

        logging.debug(f"Reading dag parameter file {dag_param_file_nm}")
        try:
            with open(dag_param_file_path) as dag_param_json:
                dag_param = json.load(dag_param_json)

                self.metadata_path = system
                self.cdf_pipeline_timeout = dag_param["cdf_pipeline_timeout"]
                self.update_metadata = dag_param["update_metadata"]
                self.refresh_authorized_views = dag_param["refresh_authorized_views"]
                self.source_type = dag_param["source_type"]

                if "load_type" not in dag_param:
                    if self.source_type == "gcs":
                        self.dag_load_type = "new_table"
                    # elif self.source_type == "relational":
                    #     self.dag_load_type = "SCD2"
                    else:
                        logging.error(
                            f"load_type is not found in dag_param, "
                            f"default load_type for {self.source_type} "
                            f"not known. Specify load_type parameter "
                            f"in DAG parameter file.")
                else:
                    self.dag_load_type = dag_param["load_type"]

                if self.env_name == "dev" or self.env_name == "test":
                    self.target_dataset = dag_param["target_dataset"]\
                        +"_"+self.env_name
                else:
                    self.target_dataset = dag_param["target_dataset"]

                if not "raw" in self.target_dataset:
                    ValueError(
                        "Target table dataset name must end with _raw, "
                        "specification in param file does not follow "
                        "naming convention. Exiting with failure.")

                if self.source_type == "gcs":
                    self.gcs_src_file_path = "data/"+system+"/"
                    self.gcs_archival_path = system

                else:
                    logging.debug("Unknown source type")
                    # raise AirflowException(
                    raise ValueError(
                f"Unknown source type found during param "
                f"initialization {dag_param['source_type']}")

                # Below code block initializes parameters relevant to flat files
                # This can be extended further for relational files
                if self.source_type == "gcs":
                    # set delimiter for file
                    # order of precedence:
                    #             delimiter in task param
                    #             delimiter in dag param
                    #             default delimiter
                    if "delimiter" in dag_param:
                        self.delimiter = dag_param["delimiter"]
                    else:
                        self.delimiter = self.default_delimiter
                        logging.debug(
                            f"default delimiter overriden in dag"
                            f" param {self.delimiter}")

                    if "gcs_src_file_path" in dag_param:
                        self.gcs_src_file_path = "data/" + \
                            dag_param["gcs_src_file_path"]+"/"
                        logging.debug(
                            f"default gcs_src_file_path overriden "
                            f"in dag param {self.gcs_src_file_path}")

                    if "source_regex" in dag_param:
                        self.source_regex = dag_param["source_regex"]
        except IOError:
            logging.error(f"Error opening dag_param_file {dag_param_file_nm}")
            raise

        # Read task parameter file and set parameter values
        logging.debug(f"Reading task parameter file {task_param_file_path}")

        if os.path.exists(task_param_file_path):
            try:
                with open(task_param_file_path) as task_param_json:
                    task_params_file = json.load(task_param_json)
                    object_configs = task_params_file["object_configs"]
                    task_list = []

                    if self.source_type == "gcs":
                        for obj in object_configs:

                            if "gcs_src_file_path" in obj:
                                task_gcs_src_file_path = f'data/{obj["gcs_src_file_path"]}'
                                logging.debug(
                                    f"gcs_src_file_path also found "
                                    f"in task param {task_gcs_src_file_path}")
                            else:
                                task_gcs_src_file_path = self.gcs_src_file_path

                            if "delimiter" in obj:
                                task_delimiter = obj["delimiter"]
                                logging.debug(
                                    f"delimiter also found in "
                                    f"task param {task_delimiter}")
                            else:
                                task_delimiter = self.delimiter

                            if "filename_prefix" in obj:
                                filename_prefix = obj["filename_prefix"]
                            else:
                                filename_prefix = ""

                            cdf_pipeline_name = obj["cdf_pipeline_name"]

                            if "load_type" in obj:
                                load_type = obj["load_type"]
                            else:
                                load_type = self.dag_load_type

                            task = {
                                "gcs_src_file_path": task_gcs_src_file_path,
                                "filename_prefix": filename_prefix,
                                "delimiter": task_delimiter,
                                "cdf_pipeline_name": cdf_pipeline_name,
                                "load_type": load_type
                            }

                            logging.debug(task)
                            task_list.append(task)
                        self.task_params = task_list
                        logging.debug(self.task_params)

            except IOError:
                logging.error(
                    f"Error opening dag_task_param_file "
                    f"{task_param_file_path}")
        else:
            logging.debug(
                "Task param file not found. "
                "Defaulting any required parameters "
                "to values in DAG parameter file.")

        logging.debug("Finished intialilzing paramters from files")
