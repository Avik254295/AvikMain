# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# 
# Config for running the complete pipeline including modeling and evaluation.

# Absolute paths to non-code resources.
file_paths:
  # TODO(user): Put your absolute local path to the queries subfolder here.
  # If running in an AI Platform notebook and cloning the Google Cloud
  # Professional Services repo, probably something like
  # "/home/jupyter/professional-services/examples/cloudml-support-routing/queries"
  queries: "REPO_ROOT/queries"

# Global config used by multiple pipeline steps. Also made available
# as parameters to all SQL queries.
global:
  # The project_id contains datasets which are top-level containers to
  # organize and access tables. For more information, refer to the 
  # link: https://cloud.google.com/bigquery/docs/datasets-intro
  
  # The source project_id and dataset contain the complaints table which
  # is read and stored in the destination project_id for feature
  # extraction and training.
  # Source project and dataset configuration.
  source_project_id: "bigquery-public-data"
  source_dataset: "cfpb_complaints"
  source_table: "complaint_database"
  
  # Destination project and dataset configuration.
  # TODO(user): Enter the value of your project_id below.
  destination_project_id: ""
  # destination_dataset is created by the pipeline in BigQuery with this name.
  destination_dataset: "cfpb_complaints"

  # Clean and minor refined versions of source data.
  # cleaned_features_table - This table contains the columns - issue,
  # subissue, product and subproduct after cleaning the categories
  # nulls_removed_table - This table contains the rows after removing
  # null values in the columns - complaint_narrative and
  # company_response_to_consumer.
  # clean_table - This table contains the rows after cleaning and
  # removing nulls.
  cleaned_features_table: "complaints_clean_categories"
  nulls_removed_table: "complaints_nulls_removed"
  clean_table: "complaints_clean"
  
  # Split the data into a training set and a prediction set. The
  # training set is passed to the AutoML Tables for training
  # the model. The prediction set is used for batch prediction
  # after the model is trained.
  
  # train_predict_split - This table creates a new column called
  # "splitting" which specifies whether the row is randomly
  # assigned to the training or prediction set.
  # "features_train_table" - This contains the training
  # features that are passed into the AutoML model for training.
  # "features_predict_table" - This contains the prediction
  # features that are used for batch prediction.
  train_predict_split: "TrainPredictSplit"
  features_train_table: "FeaturesTrain"
  features_predict_table: "FeaturesPredict"

  # Output tables for predictions and failed predictions from the batch
  # prediction job, ex. numeric column recieved a string.
  predictions_table: "Predictions"
  failed_predictions_table: "FailedPredictions"

  # AutoML Parameters. It is assumed that no other datasets or models have
  # been created with these names, or the pipeline will fail. They can be
  # deleted using the client, or the UI.
  dataset_display_name: "CFPBDataset"
  model_display_name: "CFPBModel"
  automl_compute_region: "us-central1"

# Files with templated queries, in file_paths.queries.
query_files:
  remove_nulls: "remove_nulls.sql"
  clean_categories: "clean_categories.sql"
  combine_tables: "combine_tables.sql"
  train_predict_split: "train_and_predict_split.sql"
  prediction_features: "prediction_features.sql"
  training_features: "training_features.sql"

# Parameters specific to individual SQL pipeline steps.
query_params:

  remove_nulls:
    columns_with_nulls_removed:
      - "consumer_complaint_narrative"
      - "company_response_to_consumer"
  
  # Specify the percentage of data to be used for batch prediction
  train_predict_split:
    test_threshold: 0.2
      
# Training parameters for the model.
model:
  # See https://cloud.google.com/automl-tables/docs/train for more details
  # Training objective and maximum train time, early stopping is enabled.
  train_budget_hours: 1
  optimization_objective: "MINIMIZE_LOG_LOSS"
  
  # See https://cloud.google.com/automl-tables/docs/prepare for more details on
  # the target and split columns.
  # Target column to predict
  target_column: "company_response_to_consumer"
  # Split dataset into training, validation, and test.
  split_column: "splitting"

  # Columns in dataset to exclude from training, will still appear in prediction.
  # Target (model.target_column), Split (model.split_column), Weight (unused),
  # and Key column (defined below in columns) must be in the
  # exclude_columns list or create_model will raise an exception.
  exclude_columns:
    - "complaint_id"
    - "date_received"
    - "company_public_response"
    - "company_name"
    - "tags"
    - "company_response_to_consumer"
    - "consumer_consent_provided"
    - "date_sent_to_company"
    - "timely_response"
    - "consumer_disputed"
    - "splitting"

  # Define the data type, nullability, and forecasting type for every column.
  # Values for type_code: "CATEGORY", "STRING", "FLOAT64", "TIMESTAMP".
  columns:

    "product":
      type_code: "CATEGORY"
      nullable: TRUE

    "subproduct":
      type_code: "CATEGORY"
      nullable: TRUE

    "issue":
      type_code: "CATEGORY"
      nullable: TRUE

    "subissue":
      type_code: "CATEGORY"
      nullable: TRUE

    "consumer_complaint_narrative":
      type_code: "STRING"
      nullable: FALSE

    "state":
      type_code: "CATEGORY"
      nullable: TRUE

    "zip_code":
      type_code: "CATEGORY"
      nullable: TRUE

    "complaint_id":
      type_code: "FLOAT64"
      nullable: FALSE

    "company_public_response":
      type_code: "CATEGORY"
      nullable: TRUE

    "company_name":
      type_code: "CATEGORY"
      nullable: TRUE

    "tags":
      type_code: "CATEGORY"
      nullable: TRUE

    "consumer_consent_provided":
      type_code: "CATEGORY"
      nullable: TRUE

    "submitted_via":
      type_code: "CATEGORY"
      nullable: TRUE

    "company_response_to_consumer":
      type_code: "CATEGORY"
      nullable: FALSE

    "timely_response":
      type_code: "CATEGORY"
      nullable: TRUE

    "consumer_disputed":
      type_code: "CATEGORY"
      nullable: TRUE

    "splitting":
      type_code: "CATEGORY"
      nullable: FALSE
