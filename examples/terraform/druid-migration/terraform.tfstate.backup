{
    "version": 3,
    "terraform_version": "0.11.10",
    "serial": 12,
    "lineage": "0dea78f2-53c7-1390-ee72-06a808ad6066",
    "modules": [
        {
            "path": [
                "root"
            ],
            "outputs": {
                "bucket_url": {
                    "sensitive": false,
                    "type": "string",
                    "value": "gs://terraform-stage-druid-latest"
                },
                "druid_coordinator_console": {
                    "sensitive": false,
                    "type": "string",
                    "value": "http://35.226.98.4:8081"
                },
                "druid_overlord_console": {
                    "sensitive": false,
                    "type": "string",
                    "value": "http://35.184.83.55:8090"
                },
                "private_network": {
                    "sensitive": false,
                    "type": "string",
                    "value": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/addresses/druid-private-network"
                },
                "sql_ip": {
                    "sensitive": false,
                    "type": "string",
                    "value": "10.48.128.12"
                },
                "sql_name": {
                    "sensitive": false,
                    "type": "string",
                    "value": "sql-bd-druid-latest1"
                },
                "sql_password_op": {
                    "sensitive": false,
                    "type": "string",
                    "value": "VrqRtPFSLSVE3xzy"
                }
            },
            "resources": {
                "data.template_file.druid_config": {
                    "type": "template_file",
                    "depends_on": [
                        "module.cloudsql",
                        "module.zookeeper"
                    ],
                    "primary": {
                        "id": "5f45d4005e7737f919d8f451d9100b6c31730819a40fb3be54fc861779747f7d",
                        "attributes": {
                            "id": "5f45d4005e7737f919d8f451d9100b6c31730819a40fb3be54fc861779747f7d",
                            "rendered": "#\n# Licensed to Metamarkets Group Inc. (Metamarkets) under one\n# or more contributor license agreements. See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership. Metamarkets licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n#\n# Extensions\n#\n\n# This is not the full list of Druid extensions, but common ones that people often use. You may need to change this list\n# based on your particular setup.\n#druid.extensions.loadList=[\"druid-kafka-eight\", \"druid-s3-extensions\", \"druid-histogram\", \"druid-datasketches\", \"druid-lookups-cached-global\", \"mysql-metadata-storage\"]\ndruid.extensions.loadList=[\"druid-kafka-eight\",\"druid-google-extensions\", \"mysql-metadata-storage\"]\n# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory\n# and uncomment the line below to point to your directory.\n#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies\n\n#\n# Logging\n#\n\n# Log all runtime properties on startup. Disable to avoid logging properties on startup:\ndruid.startup.logging.logProperties=true\n\n#\n# Zookeeper\n#\n\ndruid.zk.service.host=10.128.0.41:2181,10.128.0.43:2181,10.128.0.55:2181\ndruid.zk.paths.base=/druid\n\n#\n# Metadata storage\n#\n\n# For Derby server on your Druid Coordinator (only viable in a cluster with a single Coordinator, no fail-over):\n#druid.metadata.storage.type=derby\n#druid.metadata.storage.connector.connectURI=jdbc:derby://metadata.store.ip:1527/var/druid/metadata.db;create=true\n#druid.metadata.storage.connector.host=metadata.store.ip\n#druid.metadata.storage.connector.port=1527\n#Enable sql queries for druid\ndruid.sql.enable = true\n\n# For MySQL:\n # druid.extensions.loadList=[\"mysql-metadata-storage\"]\n  druid.metadata.storage.type=mysql\n  druid.metadata.storage.connector.connectURI=jdbc:mysql://10.48.128.12:3306/druid\n  druid.metadata.storage.connector.user=druid\n  druid.metadata.storage.connector.password=VrqRtPFSLSVE3xzy\n# For PostgreSQL (make sure to additionally include the Postgres extension):\n#druid.metadata.storage.type=postgresql\n#druid.metadata.storage.connector.connectURI=jdbc:postgresql://db.example.com:5432/druid\n#druid.metadata.storage.connector.user=...\n#druid.metadata.storage.connector.password=...\n\n#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.storage.type=hdfs\n#druid.storage.storageDirectory=gs://druid-migration/druid/segments\n#druid.extensions.coordinates=[\"io.druid.extensions:druid-google-extensions:0.13.0\"]\n#druid.storage.type=google\n#druid.extensions.loadList=[\"druid-google-extensions\"]\ndruid.storage.type=google\ndruid.google.bucket=terraform-stage-druid-latest\ndruid.google.prefix=druid/segments\n# For S3:\n#druid.storage.type=s3\n#druid.storage.bucket=your-bucket\n#druid.storage.baseKey=druid/segments\n#druid.s3.accessKey=...\n#druid.s3.secretKey=...\n\n#\n# Indexing service logs\n#\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=gs://druid-migration/druid/indexing-logs\ndruid.indexer.logs.type=google\ndruid.indexer.logs.bucket=terraform-stage-druid-latest\ndruid.indexer.logs.prefix=var/\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=/druid/indexing-logs\n\n# For S3:\n#druid.indexer.logs.type=s3\n#druid.indexer.logs.s3Bucket=your-bucket\n#druid.indexer.logs.s3Prefix=druid/indexing-logs\n\n#\n# Service discovery\n#\n\ndruid.selectors.indexing.serviceName=druid/overlord\ndruid.selectors.coordinator.serviceName=druid/coordinator\n\n#\n# Monitoring\n#\n\ndruid.monitoring.monitors=[\"io.druid.java.util.metrics.JvmMonitor\"]\ndruid.emitter=logging\ndruid.emitter.logging.logLevel=info\n\n# Storage type of double columns\n# ommiting this will lead to index double as float at the storage layer\n\ndruid.indexing.doubleStorage=double\n",
                            "template": "#\n# Licensed to Metamarkets Group Inc. (Metamarkets) under one\n# or more contributor license agreements. See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership. Metamarkets licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n#\n# Extensions\n#\n\n# This is not the full list of Druid extensions, but common ones that people often use. You may need to change this list\n# based on your particular setup.\n#druid.extensions.loadList=[\"druid-kafka-eight\", \"druid-s3-extensions\", \"druid-histogram\", \"druid-datasketches\", \"druid-lookups-cached-global\", \"mysql-metadata-storage\"]\ndruid.extensions.loadList=[\"druid-kafka-eight\",\"druid-google-extensions\", \"mysql-metadata-storage\"]\n# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory\n# and uncomment the line below to point to your directory.\n#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies\n\n#\n# Logging\n#\n\n# Log all runtime properties on startup. Disable to avoid logging properties on startup:\ndruid.startup.logging.logProperties=true\n\n#\n# Zookeeper\n#\n\ndruid.zk.service.host=${zookeper_ip}\ndruid.zk.paths.base=/druid\n\n#\n# Metadata storage\n#\n\n# For Derby server on your Druid Coordinator (only viable in a cluster with a single Coordinator, no fail-over):\n#druid.metadata.storage.type=derby\n#druid.metadata.storage.connector.connectURI=jdbc:derby://metadata.store.ip:1527/var/druid/metadata.db;create=true\n#druid.metadata.storage.connector.host=metadata.store.ip\n#druid.metadata.storage.connector.port=1527\n#Enable sql queries for druid\ndruid.sql.enable = true\n\n# For MySQL:\n # druid.extensions.loadList=[\"mysql-metadata-storage\"]\n  druid.metadata.storage.type=mysql\n  druid.metadata.storage.connector.connectURI=jdbc:mysql://${cloudsql_ip}:3306/${db_name}\n  druid.metadata.storage.connector.user=${username}\n  druid.metadata.storage.connector.password=${password}\n# For PostgreSQL (make sure to additionally include the Postgres extension):\n#druid.metadata.storage.type=postgresql\n#druid.metadata.storage.connector.connectURI=jdbc:postgresql://db.example.com:5432/druid\n#druid.metadata.storage.connector.user=...\n#druid.metadata.storage.connector.password=...\n\n#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.storage.type=hdfs\n#druid.storage.storageDirectory=gs://druid-migration/druid/segments\n#druid.extensions.coordinates=[\"io.druid.extensions:druid-google-extensions:0.13.0\"]\n#druid.storage.type=google\n#druid.extensions.loadList=[\"druid-google-extensions\"]\ndruid.storage.type=google\ndruid.google.bucket=${bucket_name}\ndruid.google.prefix=druid/segments\n# For S3:\n#druid.storage.type=s3\n#druid.storage.bucket=your-bucket\n#druid.storage.baseKey=druid/segments\n#druid.s3.accessKey=...\n#druid.s3.secretKey=...\n\n#\n# Indexing service logs\n#\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=gs://druid-migration/druid/indexing-logs\ndruid.indexer.logs.type=google\ndruid.indexer.logs.bucket=${bucket_name}\ndruid.indexer.logs.prefix=var/\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=/druid/indexing-logs\n\n# For S3:\n#druid.indexer.logs.type=s3\n#druid.indexer.logs.s3Bucket=your-bucket\n#druid.indexer.logs.s3Prefix=druid/indexing-logs\n\n#\n# Service discovery\n#\n\ndruid.selectors.indexing.serviceName=druid/overlord\ndruid.selectors.coordinator.serviceName=druid/coordinator\n\n#\n# Monitoring\n#\n\ndruid.monitoring.monitors=[\"io.druid.java.util.metrics.JvmMonitor\"]\ndruid.emitter=logging\ndruid.emitter.logging.logLevel=info\n\n# Storage type of double columns\n# ommiting this will lead to index double as float at the storage layer\n\ndruid.indexing.doubleStorage=double\n",
                            "vars.%": "6",
                            "vars.bucket_name": "terraform-stage-druid-latest",
                            "vars.cloudsql_ip": "10.48.128.12",
                            "vars.db_name": "druid",
                            "vars.password": "VrqRtPFSLSVE3xzy",
                            "vars.username": "druid",
                            "vars.zookeper_ip": "10.128.0.41:2181,10.128.0.43:2181,10.128.0.55:2181"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_firewall.druid_firewall": {
                    "type": "google_compute_firewall",
                    "depends_on": [],
                    "primary": {
                        "id": "allow-druid-gui",
                        "attributes": {
                            "allow.#": "1",
                            "allow.815782900.ports.#": "5",
                            "allow.815782900.ports.0": "8081",
                            "allow.815782900.ports.1": "8082",
                            "allow.815782900.ports.2": "8083",
                            "allow.815782900.ports.3": "8090",
                            "allow.815782900.ports.4": "8091",
                            "allow.815782900.protocol": "tcp",
                            "creation_timestamp": "2019-01-23T05:59:07.395-08:00",
                            "deny.#": "0",
                            "description": "",
                            "destination_ranges.#": "0",
                            "direction": "INGRESS",
                            "disabled": "false",
                            "enable_logging": "false",
                            "id": "allow-druid-gui",
                            "name": "allow-druid-gui",
                            "network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "priority": "1000",
                            "project": "pso-druid-presto-migration",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/firewalls/allow-druid-gui",
                            "source_ranges.#": "1",
                            "source_ranges.1080289494": "0.0.0.0/0",
                            "source_service_accounts.#": "0",
                            "source_tags.#": "0",
                            "target_service_accounts.#": "0",
                            "target_tags.#": "1",
                            "target_tags.3346829715": "druid"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 240000000000,
                                "delete": 240000000000,
                                "update": 240000000000
                            },
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_storage_bucket_object.common-properties": {
                    "type": "google_storage_bucket_object",
                    "depends_on": [
                        "data.template_file.druid_config"
                    ],
                    "primary": {
                        "id": "terraform-stage-druid-latest-common.runtime.properties",
                        "attributes": {
                            "bucket": "terraform-stage-druid-latest",
                            "cache_control": "",
                            "content": "#\n# Licensed to Metamarkets Group Inc. (Metamarkets) under one\n# or more contributor license agreements. See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership. Metamarkets licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n#\n# Extensions\n#\n\n# This is not the full list of Druid extensions, but common ones that people often use. You may need to change this list\n# based on your particular setup.\n#druid.extensions.loadList=[\"druid-kafka-eight\", \"druid-s3-extensions\", \"druid-histogram\", \"druid-datasketches\", \"druid-lookups-cached-global\", \"mysql-metadata-storage\"]\ndruid.extensions.loadList=[\"druid-kafka-eight\",\"druid-google-extensions\", \"mysql-metadata-storage\"]\n# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory\n# and uncomment the line below to point to your directory.\n#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies\n\n#\n# Logging\n#\n\n# Log all runtime properties on startup. Disable to avoid logging properties on startup:\ndruid.startup.logging.logProperties=true\n\n#\n# Zookeeper\n#\n\ndruid.zk.service.host=10.128.0.41:2181,10.128.0.43:2181,10.128.0.55:2181\ndruid.zk.paths.base=/druid\n\n#\n# Metadata storage\n#\n\n# For Derby server on your Druid Coordinator (only viable in a cluster with a single Coordinator, no fail-over):\n#druid.metadata.storage.type=derby\n#druid.metadata.storage.connector.connectURI=jdbc:derby://metadata.store.ip:1527/var/druid/metadata.db;create=true\n#druid.metadata.storage.connector.host=metadata.store.ip\n#druid.metadata.storage.connector.port=1527\n#Enable sql queries for druid\ndruid.sql.enable = true\n\n# For MySQL:\n # druid.extensions.loadList=[\"mysql-metadata-storage\"]\n  druid.metadata.storage.type=mysql\n  druid.metadata.storage.connector.connectURI=jdbc:mysql://10.48.128.12:3306/druid\n  druid.metadata.storage.connector.user=druid\n  druid.metadata.storage.connector.password=VrqRtPFSLSVE3xzy\n# For PostgreSQL (make sure to additionally include the Postgres extension):\n#druid.metadata.storage.type=postgresql\n#druid.metadata.storage.connector.connectURI=jdbc:postgresql://db.example.com:5432/druid\n#druid.metadata.storage.connector.user=...\n#druid.metadata.storage.connector.password=...\n\n#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.storage.type=hdfs\n#druid.storage.storageDirectory=gs://druid-migration/druid/segments\n#druid.extensions.coordinates=[\"io.druid.extensions:druid-google-extensions:0.13.0\"]\n#druid.storage.type=google\n#druid.extensions.loadList=[\"druid-google-extensions\"]\ndruid.storage.type=google\ndruid.google.bucket=terraform-stage-druid-latest\ndruid.google.prefix=druid/segments\n# For S3:\n#druid.storage.type=s3\n#druid.storage.bucket=your-bucket\n#druid.storage.baseKey=druid/segments\n#druid.s3.accessKey=...\n#druid.s3.secretKey=...\n\n#\n# Indexing service logs\n#\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=gs://druid-migration/druid/indexing-logs\ndruid.indexer.logs.type=google\ndruid.indexer.logs.bucket=terraform-stage-druid-latest\ndruid.indexer.logs.prefix=var/\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS (make sure to include the HDFS extension and that your Hadoop config files in the cp):\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=/druid/indexing-logs\n\n# For S3:\n#druid.indexer.logs.type=s3\n#druid.indexer.logs.s3Bucket=your-bucket\n#druid.indexer.logs.s3Prefix=druid/indexing-logs\n\n#\n# Service discovery\n#\n\ndruid.selectors.indexing.serviceName=druid/overlord\ndruid.selectors.coordinator.serviceName=druid/coordinator\n\n#\n# Monitoring\n#\n\ndruid.monitoring.monitors=[\"io.druid.java.util.metrics.JvmMonitor\"]\ndruid.emitter=logging\ndruid.emitter.logging.logLevel=info\n\n# Storage type of double columns\n# ommiting this will lead to index double as float at the storage layer\n\ndruid.indexing.doubleStorage=double\n",
                            "content_disposition": "",
                            "content_encoding": "",
                            "content_language": "",
                            "content_type": "text/plain; charset=utf-8",
                            "crc32c": "FEAYpQ==",
                            "detect_md5hash": "aGC/EZaU6AJ5eTA09AT3SQ==",
                            "id": "terraform-stage-druid-latest-common.runtime.properties",
                            "md5hash": "aGC/EZaU6AJ5eTA09AT3SQ==",
                            "name": "common.runtime.properties",
                            "storage_class": "STANDARD"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "cloudsql"
            ],
            "outputs": {
                "bucket_url": {
                    "sensitive": false,
                    "type": "string",
                    "value": "gs://terraform-stage-druid-latest"
                },
                "instance_address": {
                    "sensitive": false,
                    "type": "string",
                    "value": "10.48.128.12"
                },
                "instance_address_time_to_retire": {
                    "sensitive": false,
                    "type": "string",
                    "value": ""
                },
                "instance_name": {
                    "sensitive": false,
                    "type": "string",
                    "value": "sql-bd-druid-latest1"
                },
                "private_network_url": {
                    "sensitive": false,
                    "type": "string",
                    "value": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/addresses/druid-private-network"
                },
                "self_link": {
                    "sensitive": false,
                    "type": "string",
                    "value": "https://www.googleapis.com/sql/v1beta4/projects/pso-druid-presto-migration/instances/sql-bd-druid-latest1"
                },
                "sql_password": {
                    "sensitive": true,
                    "type": "string",
                    "value": "VrqRtPFSLSVE3xzy"
                }
            },
            "resources": {
                "data.template_file.dataproc-init": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "3ed9dfa43a2c26a7d287df5b17a9f480a35cfab354797558ede458c847f75fc9",
                        "attributes": {
                            "id": "3ed9dfa43a2c26a7d287df5b17a9f480a35cfab354797558ede458c847f75fc9",
                            "rendered": "#!/bin/bash\nROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)\nif [[ \"${ROLE}\" == 'Master' ]]; then\n  gsutil cp /etc/hadoop/conf/core-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/hdfs-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/mapred-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/yarn-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /usr/lib/hadoop/lib/gcs-connector-hadoop2* gs://terraform-stage-druid-latest/config/\nfi",
                            "template": "#!/bin/bash\nROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)\nif [[ \"$${ROLE}\" == 'Master' ]]; then\n  gsutil cp /etc/hadoop/conf/core-site.xml gs://${bucket}/config/\n  gsutil cp /etc/hadoop/conf/hdfs-site.xml gs://${bucket}/config/\n  gsutil cp /etc/hadoop/conf/mapred-site.xml gs://${bucket}/config/\n  gsutil cp /etc/hadoop/conf/yarn-site.xml gs://${bucket}/config/\n  gsutil cp /usr/lib/hadoop/lib/gcs-connector-hadoop2* gs://${bucket}/config/\nfi",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_global_address.private_ip_alloc": {
                    "type": "google_compute_global_address",
                    "depends_on": [],
                    "primary": {
                        "id": "druid-private-network",
                        "attributes": {
                            "address": "10.122.176.0",
                            "address_type": "INTERNAL",
                            "creation_timestamp": "2019-01-23T05:38:10.169-08:00",
                            "description": "",
                            "id": "druid-private-network",
                            "ip_version": "",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "name": "druid-private-network",
                            "network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "prefix_length": "20",
                            "project": "pso-druid-presto-migration",
                            "purpose": "VPC_PEERING",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/addresses/druid-private-network"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 240000000000,
                                "delete": 240000000000,
                                "update": 240000000000
                            }
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google-beta"
                },
                "google_service_networking_connection.googleservice": {
                    "type": "google_service_networking_connection",
                    "depends_on": [
                        "google_compute_global_address.private_ip_alloc"
                    ],
                    "primary": {
                        "id": "projects%2Fpso-druid-presto-migration%2Fglobal%2Fnetworks%2Fdefault:servicenetworking.googleapis.com",
                        "attributes": {
                            "id": "projects%2Fpso-druid-presto-migration%2Fglobal%2Fnetworks%2Fdefault:servicenetworking.googleapis.com",
                            "network": "projects/pso-druid-presto-migration/global/networks/default",
                            "reserved_peering_ranges.#": "1",
                            "reserved_peering_ranges.0": "druid-private-network",
                            "service": "servicenetworking.googleapis.com"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google-beta"
                },
                "google_sql_database.database": {
                    "type": "google_sql_database",
                    "depends_on": [
                        "google_sql_database_instance.cloudsql"
                    ],
                    "primary": {
                        "id": "sql-bd-druid-latest1:druid",
                        "attributes": {
                            "charset": "utf8",
                            "collation": "utf8_general_ci",
                            "id": "sql-bd-druid-latest1:druid",
                            "instance": "sql-bd-druid-latest1",
                            "name": "druid",
                            "project": "pso-druid-presto-migration",
                            "self_link": "https://www.googleapis.com/sql/v1beta4/projects/pso-druid-presto-migration/instances/sql-bd-druid-latest1/databases/druid"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 900000000000,
                                "delete": 600000000000,
                                "update": 600000000000
                            }
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_sql_database_instance.cloudsql": {
                    "type": "google_sql_database_instance",
                    "depends_on": [
                        "google_compute_global_address.private_ip_alloc",
                        "google_service_networking_connection.googleservice"
                    ],
                    "primary": {
                        "id": "sql-bd-druid-latest1",
                        "attributes": {
                            "connection_name": "pso-druid-presto-migration:us-central1:sql-bd-druid-latest1",
                            "database_version": "MYSQL_5_7",
                            "first_ip_address": "10.48.128.12",
                            "id": "sql-bd-druid-latest1",
                            "ip_address.#": "1",
                            "ip_address.0.ip_address": "10.48.128.12",
                            "ip_address.0.time_to_retire": "",
                            "master_instance_name": "",
                            "name": "sql-bd-druid-latest1",
                            "project": "pso-druid-presto-migration",
                            "region": "us-central1",
                            "replica_configuration.#": "0",
                            "self_link": "https://www.googleapis.com/sql/v1beta4/projects/pso-druid-presto-migration/instances/sql-bd-druid-latest1",
                            "server_ca_cert.#": "1",
                            "server_ca_cert.0.cert": "-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIBADANBgkqhkiG9w0BAQsFADBIMSMwIQYDVQQDExpHb29n\nbGUgQ2xvdWQgU1FMIFNlcnZlciBDQTEUMBIGA1UEChMLR29vZ2xlLCBJbmMxCzAJ\nBgNVBAYTAlVTMB4XDTE5MDEyMzEzNDAwN1oXDTI5MDEyMDEzNDEwN1owSDEjMCEG\nA1UEAxMaR29vZ2xlIENsb3VkIFNRTCBTZXJ2ZXIgQ0ExFDASBgNVBAoTC0dvb2ds\nZSwgSW5jMQswCQYDVQQGEwJVUzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC\nggEBAJOmbPKVbKs1en2u5V5xCEy2SoINAUkjetl2/q0UGGHeQs47B/aqc0lV4gcy\nJIkD9FP14JvRF7WrckLoMgEyxfGgaprR5pw79CwtYm8K9r0tW1LT8QrmTGPtvJMj\nl3mzKjtOnsFHEYkSt0hr4+Rm6kwJHVh6yJu+9qJfXZotdDN9/bFJmUCJBQYWik27\n8zuYz3Tgp88AURrZzFsT2xNfyhQFdUVwwzO+ik/I6chT/rVd0ngBs0dZnBdxDyDs\nMzUaIecXqF0g+J8rU4DeJhFqBO/a7yW71RvWgHQbvEQKDn7z9Avpi7Layf2D67kV\nRh2s2rrXWm3jsKWR5GH7X/U1GcsCAwEAAaMWMBQwEgYDVR0TAQH/BAgwBgEB/wIB\nADANBgkqhkiG9w0BAQsFAAOCAQEAXgiBZGteZt6EHfjQDqDhqVflS/pgJqvoy6cA\nUw/yYq8XXDH6Qy+JzTtDV07J1miJhVGOVIrNOYxAVGN0W+3yZIf2drIRiak2JC8g\nBSER3ChfP1uYQxvJVjdYsbsAOJh3t3u47l65qKA2/O9mBo0BQqsREUlWjKTEomZf\nA5dSF7zm6lFiBOiTKWj/sb4HBSIW7QQnmUMjTsRHPFSOWpY7Ko+rX29Lu1gbumdw\nOOYuznTnjqmRzG2M5AU5+b3gM8tCaiiNglbctZkj+p2S2rjtexwNb936KyTkKFTa\nDttyYttr5DJJeQp51TsKbJK5VJ2AfaKruSk+yPCj7HdMjQrGdw==\n-----END CERTIFICATE-----",
                            "server_ca_cert.0.common_name": "C=US,O=Google\\, Inc,CN=Google Cloud SQL Server CA",
                            "server_ca_cert.0.create_time": "2019-01-23T13:40:07.034Z",
                            "server_ca_cert.0.expiration_time": "2029-01-20T13:41:07.034Z",
                            "server_ca_cert.0.sha1_fingerprint": "226c6b877e3a4326440424ba6660464711159c8f",
                            "service_account_email_address": "e32ldhzefjcrreotfiytkco72q@speckle-umbrella-28.iam.gserviceaccount.com",
                            "settings.#": "1",
                            "settings.0.activation_policy": "ALWAYS",
                            "settings.0.authorized_gae_applications.#": "0",
                            "settings.0.availability_type": "",
                            "settings.0.backup_configuration.#": "1",
                            "settings.0.backup_configuration.0.binary_log_enabled": "false",
                            "settings.0.backup_configuration.0.enabled": "false",
                            "settings.0.backup_configuration.0.start_time": "10:00",
                            "settings.0.crash_safe_replication": "false",
                            "settings.0.database_flags.#": "0",
                            "settings.0.disk_autoresize": "true",
                            "settings.0.disk_size": "10",
                            "settings.0.disk_type": "PD_SSD",
                            "settings.0.ip_configuration.#": "1",
                            "settings.0.ip_configuration.0.authorized_networks.#": "0",
                            "settings.0.ip_configuration.0.ipv4_enabled": "false",
                            "settings.0.ip_configuration.0.private_network": "projects/pso-druid-presto-migration/global/networks/default",
                            "settings.0.ip_configuration.0.require_ssl": "false",
                            "settings.0.location_preference.#": "0",
                            "settings.0.maintenance_window.#": "0",
                            "settings.0.pricing_plan": "PER_USE",
                            "settings.0.replication_type": "SYNCHRONOUS",
                            "settings.0.tier": "db-f1-micro",
                            "settings.0.user_labels.%": "0",
                            "settings.0.version": "2"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 600000000000,
                                "delete": 600000000000,
                                "update": 600000000000
                            }
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_sql_user.default": {
                    "type": "google_sql_user",
                    "depends_on": [
                        "google_sql_database_instance.cloudsql",
                        "random_string.password"
                    ],
                    "primary": {
                        "id": "druid/%/sql-bd-druid-latest1",
                        "attributes": {
                            "host": "%",
                            "id": "druid/%/sql-bd-druid-latest1",
                            "instance": "sql-bd-druid-latest1",
                            "name": "druid",
                            "password": "VrqRtPFSLSVE3xzy",
                            "project": "pso-druid-presto-migration"
                        },
                        "meta": {
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_storage_bucket.staging-bucket": {
                    "type": "google_storage_bucket",
                    "depends_on": [],
                    "primary": {
                        "id": "terraform-stage-druid-latest",
                        "attributes": {
                            "cors.#": "0",
                            "encryption.#": "0",
                            "force_destroy": "true",
                            "id": "terraform-stage-druid-latest",
                            "labels.%": "0",
                            "lifecycle_rule.#": "0",
                            "location": "US-CENTRAL1",
                            "logging.#": "0",
                            "name": "terraform-stage-druid-latest",
                            "project": "pso-druid-presto-migration",
                            "self_link": "https://www.googleapis.com/storage/v1/b/terraform-stage-druid-latest",
                            "storage_class": "STANDARD",
                            "url": "gs://terraform-stage-druid-latest",
                            "versioning.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_storage_bucket_object.init-script": {
                    "type": "google_storage_bucket_object",
                    "depends_on": [
                        "data.template_file.dataproc-init",
                        "google_storage_bucket.staging-bucket"
                    ],
                    "primary": {
                        "id": "terraform-stage-druid-latest-dataproc_init.sh",
                        "attributes": {
                            "bucket": "terraform-stage-druid-latest",
                            "cache_control": "",
                            "content": "#!/bin/bash\nROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)\nif [[ \"${ROLE}\" == 'Master' ]]; then\n  gsutil cp /etc/hadoop/conf/core-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/hdfs-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/mapred-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /etc/hadoop/conf/yarn-site.xml gs://terraform-stage-druid-latest/config/\n  gsutil cp /usr/lib/hadoop/lib/gcs-connector-hadoop2* gs://terraform-stage-druid-latest/config/\nfi",
                            "content_disposition": "",
                            "content_encoding": "",
                            "content_language": "",
                            "content_type": "text/plain; charset=utf-8",
                            "crc32c": "cdyI9w==",
                            "detect_md5hash": "to+3+C+1Zruu0BqjkhiZ0g==",
                            "id": "terraform-stage-druid-latest-dataproc_init.sh",
                            "md5hash": "to+3+C+1Zruu0BqjkhiZ0g==",
                            "name": "dataproc_init.sh",
                            "storage_class": "STANDARD"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "null_resource.patch_sql": {
                    "type": "null_resource",
                    "depends_on": [
                        "google_compute_global_address.private_ip_alloc",
                        "google_service_networking_connection.googleservice",
                        "google_sql_database.database",
                        "google_sql_database_instance.cloudsql",
                        "google_sql_user.default"
                    ],
                    "primary": {
                        "id": "8685900068736597717",
                        "attributes": {
                            "id": "8685900068736597717"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "random_string.password": {
                    "type": "random_string",
                    "depends_on": [],
                    "primary": {
                        "id": "none",
                        "attributes": {
                            "id": "none",
                            "length": "16",
                            "lower": "true",
                            "min_lower": "0",
                            "min_numeric": "0",
                            "min_special": "0",
                            "min_upper": "0",
                            "number": "true",
                            "result": "VrqRtPFSLSVE3xzy",
                            "special": "false",
                            "upper": "true"
                        },
                        "meta": {
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.random"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "dataproc"
            ],
            "outputs": {
                "master_name_op": {
                    "sensitive": false,
                    "type": "string",
                    "value": "dataproc-terraform-druid-latest-m"
                },
                "worker_name_op": {
                    "sensitive": false,
                    "type": "string",
                    "value": "dataproc-terraform-druid-latest-w-0"
                }
            },
            "resources": {
                "google_dataproc_cluster.ephemeral-dataproc": {
                    "type": "google_dataproc_cluster",
                    "depends_on": [],
                    "primary": {
                        "id": "dataproc-terraform-druid-latest",
                        "attributes": {
                            "cluster_config.#": "1",
                            "cluster_config.0.bucket": "terraform-stage-druid-latest",
                            "cluster_config.0.delete_autogen_bucket": "false",
                            "cluster_config.0.gce_cluster_config.#": "1",
                            "cluster_config.0.gce_cluster_config.0.internal_ip_only": "false",
                            "cluster_config.0.gce_cluster_config.0.metadata.%": "0",
                            "cluster_config.0.gce_cluster_config.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "cluster_config.0.gce_cluster_config.0.service_account": "",
                            "cluster_config.0.gce_cluster_config.0.service_account_scopes.#": "4",
                            "cluster_config.0.gce_cluster_config.0.service_account_scopes.1328717722": "https://www.googleapis.com/auth/devstorage.read_write",
                            "cluster_config.0.gce_cluster_config.0.service_account_scopes.172152165": "https://www.googleapis.com/auth/logging.write",
                            "cluster_config.0.gce_cluster_config.0.service_account_scopes.3804780973": "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
                            "cluster_config.0.gce_cluster_config.0.service_account_scopes.4205865871": "https://www.googleapis.com/auth/sqlservice.admin",
                            "cluster_config.0.gce_cluster_config.0.subnetwork": "",
                            "cluster_config.0.gce_cluster_config.0.tags.#": "0",
                            "cluster_config.0.gce_cluster_config.0.zone": "us-central1-c",
                            "cluster_config.0.initialization_action.#": "1",
                            "cluster_config.0.initialization_action.0.script": "gs://terraform-stage-druid-latest/dataproc_init.sh",
                            "cluster_config.0.initialization_action.0.timeout_sec": "500",
                            "cluster_config.0.master_config.#": "1",
                            "cluster_config.0.master_config.0.disk_config.#": "1",
                            "cluster_config.0.master_config.0.disk_config.0.boot_disk_size_gb": "200",
                            "cluster_config.0.master_config.0.disk_config.0.boot_disk_type": "pd-ssd",
                            "cluster_config.0.master_config.0.disk_config.0.num_local_ssds": "0",
                            "cluster_config.0.master_config.0.instance_names.#": "1",
                            "cluster_config.0.master_config.0.instance_names.0": "dataproc-terraform-druid-latest-m",
                            "cluster_config.0.master_config.0.machine_type": "n1-standard-8",
                            "cluster_config.0.master_config.0.num_instances": "1",
                            "cluster_config.0.preemptible_worker_config.#": "1",
                            "cluster_config.0.preemptible_worker_config.0.disk_config.#": "1",
                            "cluster_config.0.preemptible_worker_config.0.instance_names.#": "0",
                            "cluster_config.0.preemptible_worker_config.0.num_instances": "0",
                            "cluster_config.0.software_config.#": "1",
                            "cluster_config.0.software_config.0.image_version": "1.3.7-deb9",
                            "cluster_config.0.software_config.0.override_properties.%": "0",
                            "cluster_config.0.software_config.0.properties.%": "52",
                            "cluster_config.0.software_config.0.properties.capacity-scheduler:yarn.scheduler.capacity.root.default.ordering-policy": "fair",
                            "cluster_config.0.software_config.0.properties.core:fs.gs.block.size": "134217728",
                            "cluster_config.0.software_config.0.properties.core:fs.gs.metadata.cache.enable": "false",
                            "cluster_config.0.software_config.0.properties.core:hadoop.ssl.enabled.protocols": "TLSv1,TLSv1.1,TLSv1.2",
                            "cluster_config.0.software_config.0.properties.distcp:mapreduce.map.java.opts": "-Xmx768m",
                            "cluster_config.0.software_config.0.properties.distcp:mapreduce.map.memory.mb": "1024",
                            "cluster_config.0.software_config.0.properties.distcp:mapreduce.reduce.java.opts": "-Xmx768m",
                            "cluster_config.0.software_config.0.properties.distcp:mapreduce.reduce.memory.mb": "1024",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.datanode.address": "0.0.0.0:9866",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.datanode.http.address": "0.0.0.0:9864",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.datanode.https.address": "0.0.0.0:9865",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.datanode.ipc.address": "0.0.0.0:9867",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.handler.count": "60",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.http-address": "0.0.0.0:9870",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.https-address": "0.0.0.0:9871",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.lifeline.rpc-address": "dataproc-terraform-druid-latest-m:8050",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.secondary.http-address": "0.0.0.0:9868",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.secondary.https-address": "0.0.0.0:9869",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.service.handler.count": "30",
                            "cluster_config.0.software_config.0.properties.hdfs:dfs.namenode.servicerpc-address": "dataproc-terraform-druid-latest-m:8051",
                            "cluster_config.0.software_config.0.properties.mapred-env:HADOOP_JOB_HISTORYSERVER_HEAPSIZE": "4000",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.job.maps": "573",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.job.reduce.slowstart.completedmaps": "0.95",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.job.reduces": "191",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.map.cpu.vcores": "1",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.map.java.opts": "-Xmx2457m",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.map.memory.mb": "3072",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.reduce.cpu.vcores": "1",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.reduce.java.opts": "-Xmx2457m",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.reduce.memory.mb": "3072",
                            "cluster_config.0.software_config.0.properties.mapred:mapreduce.task.io.sort.mb": "256",
                            "cluster_config.0.software_config.0.properties.mapred:yarn.app.mapreduce.am.command-opts": "-Xmx2457m",
                            "cluster_config.0.software_config.0.properties.mapred:yarn.app.mapreduce.am.resource.cpu-vcores": "1",
                            "cluster_config.0.software_config.0.properties.mapred:yarn.app.mapreduce.am.resource.mb": "3072",
                            "cluster_config.0.software_config.0.properties.presto-jvm:MaxHeapSize": "98304m",
                            "cluster_config.0.software_config.0.properties.presto:query.max-memory-per-node": "58982MB",
                            "cluster_config.0.software_config.0.properties.presto:query.max-total-memory-per-node": "58982MB",
                            "cluster_config.0.software_config.0.properties.spark-env:SPARK_DAEMON_MEMORY": "4000m",
                            "cluster_config.0.software_config.0.properties.spark:spark.driver.maxResultSize": "3840m",
                            "cluster_config.0.software_config.0.properties.spark:spark.driver.memory": "7680m",
                            "cluster_config.0.software_config.0.properties.spark:spark.executor.cores": "8",
                            "cluster_config.0.software_config.0.properties.spark:spark.executor.instances": "2",
                            "cluster_config.0.software_config.0.properties.spark:spark.executor.memory": "22342m",
                            "cluster_config.0.software_config.0.properties.spark:spark.executorEnv.OPENBLAS_NUM_THREADS": "1",
                            "cluster_config.0.software_config.0.properties.spark:spark.scheduler.mode": "FAIR",
                            "cluster_config.0.software_config.0.properties.spark:spark.sql.cbo.enabled": "true",
                            "cluster_config.0.software_config.0.properties.spark:spark.yarn.am.memory": "640m",
                            "cluster_config.0.software_config.0.properties.yarn-env:YARN_TIMELINESERVER_HEAPSIZE": "4000",
                            "cluster_config.0.software_config.0.properties.yarn:yarn.nodemanager.resource.memory-mb": "98304",
                            "cluster_config.0.software_config.0.properties.yarn:yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs": "86400",
                            "cluster_config.0.software_config.0.properties.yarn:yarn.scheduler.maximum-allocation-mb": "98304",
                            "cluster_config.0.software_config.0.properties.yarn:yarn.scheduler.minimum-allocation-mb": "1024",
                            "cluster_config.0.staging_bucket": "terraform-stage-druid-latest",
                            "cluster_config.0.worker_config.#": "1",
                            "cluster_config.0.worker_config.0.disk_config.#": "1",
                            "cluster_config.0.worker_config.0.disk_config.0.boot_disk_size_gb": "500",
                            "cluster_config.0.worker_config.0.disk_config.0.boot_disk_type": "pd-standard",
                            "cluster_config.0.worker_config.0.disk_config.0.num_local_ssds": "0",
                            "cluster_config.0.worker_config.0.instance_names.#": "6",
                            "cluster_config.0.worker_config.0.instance_names.0": "dataproc-terraform-druid-latest-w-0",
                            "cluster_config.0.worker_config.0.instance_names.1": "dataproc-terraform-druid-latest-w-1",
                            "cluster_config.0.worker_config.0.instance_names.2": "dataproc-terraform-druid-latest-w-2",
                            "cluster_config.0.worker_config.0.instance_names.3": "dataproc-terraform-druid-latest-w-3",
                            "cluster_config.0.worker_config.0.instance_names.4": "dataproc-terraform-druid-latest-w-4",
                            "cluster_config.0.worker_config.0.instance_names.5": "dataproc-terraform-druid-latest-w-5",
                            "cluster_config.0.worker_config.0.machine_type": "n1-standard-32",
                            "cluster_config.0.worker_config.0.num_instances": "6",
                            "id": "dataproc-terraform-druid-latest",
                            "labels.%": "3",
                            "labels.goog-dataproc-cluster-name": "dataproc-terraform-druid-latest",
                            "labels.goog-dataproc-cluster-uuid": "d13fd189-9465-40bc-bbf4-107c887e1e28",
                            "labels.goog-dataproc-location": "us-central1",
                            "name": "dataproc-terraform-druid-latest",
                            "project": "pso-druid-presto-migration",
                            "region": "us-central1"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 900000000000,
                                "delete": 300000000000,
                                "update": 300000000000
                            }
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_broker"
            ],
            "outputs": {},
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "bc81c824f0c7900f879d55b0f5fe8c36d4c4913b3c5da565ada0176f9eb435a5",
                        "attributes": {
                            "id": "bc81c824f0c7900f879d55b0f5fe8c36d4c4913b3c5da565ada0176f9eb435a5",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid broker with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl -L -k ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n  mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L -k ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the middleManager service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\n  cd ${DRUIDBASEDIR} \n  mkdir -p ${DRUIDLOGSDIR}\n\n sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g' \\\n    ${DRUIDCONFIGDIR}/druid/broker/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g' \\\n        ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n        's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' \\\n        ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i \\\n        's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' \\\n        ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n    java $(cat ${DRUIDCONFIGDIR}/druid/broker/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/broker:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server broker \u003e ${DRUIDLOGSDIR}/broker.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid broker with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl -L -k $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n  mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L -k $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the middleManager service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\n  cd $${DRUIDBASEDIR} \n  mkdir -p $${DRUIDLOGSDIR}\n\n sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g' \\\n    $${DRUIDCONFIGDIR}/druid/broker/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://${bucket}/var/druid/segment-cache|g' \\\n        $${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n        's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://${bucket}/var/druid/hadoop-tmp|g' \\\n        $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i \\\n        's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' \\\n        $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n    java $(cat $${DRUIDCONFIGDIR}/druid/broker/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/broker:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server broker \u003e $${DRUIDLOGSDIR}/broker.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_coordinator"
            ],
            "outputs": {
                "druid_coordinator": {
                    "sensitive": false,
                    "type": "string",
                    "value": "10.128.0.56"
                },
                "instance_ip": {
                    "sensitive": false,
                    "type": "string",
                    "value": "35.226.98.4"
                }
            },
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "16760c980c9b28ecf2381f3ce8aca88d98380448d710acdf1e4e3f6f141d0873",
                        "attributes": {
                            "id": "16760c980c9b28ecf2381f3ce8aca88d98380448d710acdf1e4e3f6f141d0873",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid coordinator with its dependencies: java8, druid package,\n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform\n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n  mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the coordinator service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\ncd ${DRUIDBASEDIR}\nmkdir -p ${DRUIDLOGSDIR}\n\n  sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g' \\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \n\n  java $(cat ${DRUIDCONFIGDIR}/druid/coordinator/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/coordinator:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server coordinator \u003e ${DRUIDLOGSDIR}/coordinator.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid coordinator with its dependencies: java8, druid package,\n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform\n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n  mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the coordinator service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\ncd $${DRUIDBASEDIR}\nmkdir -p $${DRUIDLOGSDIR}\n\n  sed -i 's|var/druid/segment-cache|gs://${bucket}/var/druid/segment-cache|g' \\\n  $${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://${bucket}/var/druid/hadoop-tmp|g' \\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' \\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \n\n  java $(cat $${DRUIDCONFIGDIR}/druid/coordinator/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/coordinator:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server coordinator \u003e $${DRUIDLOGSDIR}/coordinator.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.druid-coordinator": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "data.template_file.druid-startup"
                    ],
                    "primary": {
                        "id": "coordinator-druid-latest-1",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "200",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/coordinator-druid-latest-1",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "coordinator-druid-latest-1",
                            "instance_id": "3471446669678464452",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-2",
                            "metadata.%": "1",
                            "metadata.startup-script": "#!/bin/bash\n\n#This script is used to install druid coordinator with its dependencies: java8, druid package,\n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform\n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n  mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the coordinator service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\ncd ${DRUIDBASEDIR}\nmkdir -p ${DRUIDLOGSDIR}\n\n  sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g' \\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \n\n  java $(cat ${DRUIDCONFIGDIR}/druid/coordinator/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/coordinator:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server coordinator \u003e ${DRUIDLOGSDIR}/coordinator.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "metadata_fingerprint": "-3DdoX2dTQ4=",
                            "min_cpu_platform": "",
                            "name": "coordinator-druid-latest-1",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.226.98.4",
                            "network_interface.0.access_config.0.nat_ip": "35.226.98.4",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.56",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.56",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/coordinator-druid-latest-1",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "2",
                            "service_account.0.scopes.1328717722": "https://www.googleapis.com/auth/devstorage.read_write",
                            "service_account.0.scopes.4205865871": "https://www.googleapis.com/auth/sqlservice.admin",
                            "tags.#": "2",
                            "tags.3346829715": "druid",
                            "tags.368971370": "coordinator",
                            "tags_fingerprint": "Hmk37YXNn-k=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_historical"
            ],
            "outputs": {},
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "879e542a8d696618aaa6ee08c851b963d22cc8a7208f72e3ecd6b1af29fdd2ab",
                        "attributes": {
                            "id": "879e542a8d696618aaa6ee08c851b963d22cc8a7208f72e3ecd6b1af29fdd2ab",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid historical with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the historical service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction configure_run_druid() {\n  mkdir -p ${DRUIDLOGSDIR}\n\ncd ${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g'\\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g'\\\n\t${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/historical/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/historical:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server historical \u003e ${DRUIDLOGSDIR}/historical.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid historical with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the historical service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction configure_run_druid() {\n  mkdir -p $${DRUIDLOGSDIR}\n\ncd $${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g'\\\n  $${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://${bucket}/var/druid/segment-cache|g'\\\n\t$${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://${bucket}/var/druid/hadoop-tmp|g'\\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n  java $(cat $${DRUIDCONFIGDIR}/druid/historical/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/historical:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server historical \u003e $${DRUIDLOGSDIR}/historical.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.druid-historical.1": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "data.template_file.druid-startup"
                    ],
                    "primary": {
                        "id": "historical-druid-latest-2",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "200",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/historical-druid-latest-2",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "historical-druid-latest-2",
                            "instance_id": "6413484771923840452",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-8",
                            "metadata.%": "1",
                            "metadata.startup-script": "#!/bin/bash\n\n#This script is used to install druid historical with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the historical service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction configure_run_druid() {\n  mkdir -p ${DRUIDLOGSDIR}\n\ncd ${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g'\\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g'\\\n\t${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/historical/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/historical:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server historical \u003e ${DRUIDLOGSDIR}/historical.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "metadata_fingerprint": "1K5I1Aq4xzc=",
                            "min_cpu_platform": "",
                            "name": "historical-druid-latest-2",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.225.13.4",
                            "network_interface.0.access_config.0.nat_ip": "35.225.13.4",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.51",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.51",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/historical-druid-latest-2",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "2",
                            "service_account.0.scopes.1328717722": "https://www.googleapis.com/auth/devstorage.read_write",
                            "service_account.0.scopes.4205865871": "https://www.googleapis.com/auth/sqlservice.admin",
                            "tags.#": "2",
                            "tags.2775567619": "historical",
                            "tags.3346829715": "druid",
                            "tags_fingerprint": "wDQXhIMrMTo=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_middlemanager"
            ],
            "outputs": {
                "druid_middlemanager": {
                    "sensitive": false,
                    "type": "string",
                    "value": "10.128.0.40"
                },
                "instance_ip": {
                    "sensitive": false,
                    "type": "string",
                    "value": "35.239.189.4"
                }
            },
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "f79400f7147a088dd96ab7e5422acfc3878537791c5404f98d7f6a7749c53590",
                        "attributes": {
                            "id": "f79400f7147a088dd96ab7e5422acfc3878537791c5404f98d7f6a7749c53590",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid middleManager with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n \n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads, jvm, and buffer size\n# in runtime.properties file of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties, creates \n# additional /var/tmp folder for its operation and runs the middleManager service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\n  HADOOPVERSION=\"0.13.0\"\n\n  mkdir -p ${DRUIDLOGSDIR}\n  mkdir -p ${DRUIDBASEDIR}/var/tmp \\\n    \u0026\u0026 chmod 755 ${DRUIDBASEDIR}/var/tmp\n\n  cd ${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g' ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xmx2g|-Xmx1g|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=2560|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.server.http.numThreads=25|druid.server.http.numThreads=9|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xms64m|-Xmx512m|g' ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config \\\n    \u0026\u0026 sed -i 's|-Xmx64m|-Xmx512m|g' ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/middleManager:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server middleManager  \u003e ${DRUIDLOGSDIR}/middleManager.log 2\u003e\u00261 \u0026\n\n }\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid middleManager with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n \n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads, jvm, and buffer size\n# in runtime.properties file of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties, creates \n# additional /var/tmp folder for its operation and runs the middleManager service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\n  HADOOPVERSION=\"0.13.0\"\n\n  mkdir -p $${DRUIDLOGSDIR}\n  mkdir -p $${DRUIDBASEDIR}/var/tmp \\\n    \u0026\u0026 chmod 755 $${DRUIDBASEDIR}/var/tmp\n\n  cd $${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://${bucket}/var/druid/segment-cache|g' $${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://${bucket}/var/druid/hadoop-tmp|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xmx2g|-Xmx1g|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=2560|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.server.http.numThreads=25|druid.server.http.numThreads=9|g' $${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xms64m|-Xmx512m|g' $${DRUIDCONFIGDIR}/druid/middleManager/jvm.config \\\n    \u0026\u0026 sed -i 's|-Xmx64m|-Xmx512m|g' $${DRUIDCONFIGDIR}/druid/middleManager/jvm.config\n\n  java $(cat $${DRUIDCONFIGDIR}/druid/middleManager/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/middleManager:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server middleManager  \u003e $${DRUIDLOGSDIR}/middleManager.log 2\u003e\u00261 \u0026\n\n }\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.druid-middlemanager": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "data.template_file.druid-startup"
                    ],
                    "primary": {
                        "id": "middlemanager-druid-latest-1",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "200",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/middlemanager-druid-latest-1",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "middlemanager-druid-latest-1",
                            "instance_id": "8472933023558782404",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-8",
                            "metadata.%": "1",
                            "metadata.startup-script": "#!/bin/bash\n\n#This script is used to install druid middleManager with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n \n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads, jvm, and buffer size\n# in runtime.properties file of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties, creates \n# additional /var/tmp folder for its operation and runs the middleManager service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() {\n  HADOOPVERSION=\"0.13.0\"\n\n  mkdir -p ${DRUIDLOGSDIR}\n  mkdir -p ${DRUIDBASEDIR}/var/tmp \\\n    \u0026\u0026 chmod 755 ${DRUIDBASEDIR}/var/tmp\n\n  cd ${DRUIDBASEDIR}\n\n  sed -i 's|druid.processing.numThreads=7|druid.processing.numThreads=2|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g' ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xmx2g|-Xmx1g|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912|druid.indexer.fork.property.druid.processing.buffer.sizeBytes=2560|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|druid.server.http.numThreads=25|druid.server.http.numThreads=9|g' ${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|-Xms64m|-Xmx512m|g' ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config \\\n    \u0026\u0026 sed -i 's|-Xmx64m|-Xmx512m|g' ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/middleManager/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/middleManager:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server middleManager  \u003e ${DRUIDLOGSDIR}/middleManager.log 2\u003e\u00261 \u0026\n\n }\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "metadata_fingerprint": "beBZL0lepYo=",
                            "min_cpu_platform": "",
                            "name": "middlemanager-druid-latest-1",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.239.189.4",
                            "network_interface.0.access_config.0.nat_ip": "35.239.189.4",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.40",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.40",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/middlemanager-druid-latest-1",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "2",
                            "service_account.0.scopes.1328717722": "https://www.googleapis.com/auth/devstorage.read_write",
                            "service_account.0.scopes.4205865871": "https://www.googleapis.com/auth/sqlservice.admin",
                            "tags.#": "2",
                            "tags.2386939171": "middlemanager",
                            "tags.3346829715": "druid",
                            "tags_fingerprint": "S8L9HSjPFJY=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_overlord"
            ],
            "outputs": {
                "druid_overlord": {
                    "sensitive": false,
                    "type": "string",
                    "value": "10.128.0.52"
                },
                "instance_ip": {
                    "sensitive": false,
                    "type": "string",
                    "value": "35.184.83.55"
                }
            },
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "012d802793a05bd838cdcff457ef4f34b0dea6e6ff2a130047cbe2cd208a7801",
                        "attributes": {
                            "id": "012d802793a05bd838cdcff457ef4f34b0dea6e6ff2a130047cbe2cd208a7801",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid overlord with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and update the hadoop version to be used\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the overlord service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() { \n  mkdir -p ${DRUIDLOGSDIR}\n\n  cd ${DRUIDBASEDIR} \n\n  sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g'\\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/overlord/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/overlord:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server overlord \u003e ${DRUIDLOGSDIR}/overlord.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid overlord with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and update the hadoop version to be used\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the overlord service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() { \n  mkdir -p $${DRUIDLOGSDIR}\n\n  cd $${DRUIDBASEDIR} \n\n  sed -i 's|var/druid/segment-cache|gs://${bucket}/var/druid/segment-cache|g'\\\n  $${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://${bucket}/var/druid/hadoop-tmp|g' \\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t$${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n\n  java $(cat $${DRUIDCONFIGDIR}/druid/overlord/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/overlord:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server overlord \u003e $${DRUIDLOGSDIR}/overlord.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.druid-overlord": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "data.template_file.druid-startup"
                    ],
                    "primary": {
                        "id": "overlord-druid-latest-1",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "200",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/overlord-druid-latest-1",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "overlord-druid-latest-1",
                            "instance_id": "5378092569175903684",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-2",
                            "metadata.%": "1",
                            "metadata.startup-script": "#!/bin/bash\n\n#This script is used to install druid overlord with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and update the hadoop version to be used\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the overlord service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction configure_run_druid() { \n  mkdir -p ${DRUIDLOGSDIR}\n\n  cd ${DRUIDBASEDIR} \n\n  sed -i 's|var/druid/segment-cache|gs://terraform-stage-druid-latest/var/druid/segment-cache|g'\\\n  ${DRUIDCONFIGDIR}/druid/historical/runtime.properties \\\n    \u0026\u0026 sed -i \\\n\t's|druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp|druid.indexer.task.hadoopWorkingPath=gs://terraform-stage-druid-latest/var/druid/hadoop-tmp|g' \\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties \\\n    \u0026\u0026 sed -i 's|org.apache.hadoop:hadoop-client:2.7.3|org.apache.hadoop:hadoop-client:2.9.0|g'\\\n\t${DRUIDCONFIGDIR}/druid/middleManager/runtime.properties\n\n\n  java $(cat ${DRUIDCONFIGDIR}/druid/overlord/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/overlord:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server overlord \u003e ${DRUIDLOGSDIR}/overlord.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "metadata_fingerprint": "zTQeucUO658=",
                            "min_cpu_platform": "",
                            "name": "overlord-druid-latest-1",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.184.83.55",
                            "network_interface.0.access_config.0.nat_ip": "35.184.83.55",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.52",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.52",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/overlord-druid-latest-1",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "2",
                            "service_account.0.scopes.1328717722": "https://www.googleapis.com/auth/devstorage.read_write",
                            "service_account.0.scopes.4205865871": "https://www.googleapis.com/auth/sqlservice.admin",
                            "tags.#": "2",
                            "tags.1574428602": "overlord",
                            "tags.3346829715": "druid",
                            "tags_fingerprint": "j-YcqbB7kbA=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "druid_router"
            ],
            "outputs": {},
            "resources": {
                "data.template_file.druid-startup": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "2940ee2004d1896773bbdc94c0abe336e98a671ea386498d62eafd0d02e367c6",
                        "attributes": {
                            "id": "2940ee2004d1896773bbdc94c0abe336e98a671ea386498d62eafd0d02e367c6",
                            "rendered": "#!/bin/bash\n\n#This script is used to install druid historical with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"${DOWNLOADDIR}/apache-druid-${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p ${DOWNLOADDIR} \\\n    \u0026\u0026 curl ${DRUIDURL} -o ${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd ${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf ${DRUIDBASEDIR}-bin.tar.gz \n\n mv ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/common.runtime.properties ${DRUIDCONFIGDIR}/druid/_common/\n\n  cd ${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:${DRUIDVERSION}\"\n\n  curl -L ${MYSQLURL} -o mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-${MYSQLVER}/mysql-connector-java-${MYSQLVER}.jar ${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://terraform-stage-druid-latest/config/core-site.xml  ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/hdfs-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/mapred-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/yarn-site.xml ${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://terraform-stage-druid-latest/config/gcs-connector-hadoop2* ${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the historical service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction configure_run_druid() {\n  mkdir -p ${DRUIDLOGSDIR}\n  mkdir -p ${DRUIDCONFIGDIR}/druid/router\n  mkdir /var/tmp\n\n  cat \u003e ${DRUIDCONFIGDIR}/druid/router/jvm.config \u003c\u003c EOF\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/tmp/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/var/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\nEOF\n\n\n  cat \u003e ${DRUIDCONFIGDIR}/druid/router/runtime.properties \u003c\u003c EOF\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.tierToBrokerMap={\"_default_tier\":\"druid/broker\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the router proxy http client\ndruid.router.http.numMaxThreads=100\ndruid.server.http.numThreads=100\nEOF\n\ncd ${DRUIDBASEDIR} \n\njava $(cat ${DRUIDCONFIGDIR}/druid/router/jvm.config | xargs) -cp \"${DRUIDCONFIGDIR}/druid/_common:${DRUIDCONFIGDIR}/druid/router:lib/*:${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/${HADOOPVERSION}/*\" org.apache.druid.cli.Main server router \u003e ${DRUIDLOGSDIR}/router.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "#!/bin/bash\n\n#This script is used to install druid historical with its dependencies: java8, druid package, \n#and druid/hadoop extensions. This script has dependency on other configuration files which\n#is pulled from the bucket defined by Terraform \n\nDOWNLOADDIR=\"/opt/druid\"\nreadonly DRUIDVERSION=\"0.13.0-incubating\" # Druid version\nDRUIDBASEDIR=\"$${DOWNLOADDIR}/apache-druid-$${DRUIDVERSION}\"\nreadonly DRUIDLOGSDIR=\"$${DRUIDBASEDIR}/log\"\nreadonly DRUIDCONFIGDIR=\"$${DRUIDBASEDIR}/conf\"\nDRUIDURL=\"http://mirrors.estointernet.in/apache/incubator/druid/0.13.0-incubating/apache-druid-0.13.0-incubating-bin.tar.gz\"\nMYSQLURL=\"https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip\"\nMYSQLVER=\"5.1.47\"\nHADOOPVERSION=\"2.9.0\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction install_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk \\\n  \u0026\u0026 apt-get install -y unzip\n}\n\n#######################################\n# Downloads the druid package, backup the common.runtime.properties file and download\n# the already configured common.runtime.properties file from defined bucket by Terraform\n# It is also responsible to pull the required extensions of hadoop and druid from its repository\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_druid() {\n  mkdir -p $${DOWNLOADDIR} \\\n    \u0026\u0026 curl $${DRUIDURL} -o $${DRUIDBASEDIR}-bin.tar.gz \\\n    \u0026\u0026 cd $${DOWNLOADDIR} \\\n    \u0026\u0026 tar -xzf $${DRUIDBASEDIR}-bin.tar.gz \n\n mv $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/common.runtime.properties.backup \\\n    \u0026\u0026 gsutil cp gs://${bucket}/common.runtime.properties $${DRUIDCONFIGDIR}/druid/_common/\n\n  cd $${DRUIDBASEDIR}\n\n  java -cp \"lib/*\" \\\n      -Ddruid.extensions.directory=\"extensions\" \\\n      -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n      org.apache.druid.cli.Main tools pull-deps --no-default-hadoop --clean \\\n      -h \"org.apache.hadoop:hadoop-client:$${HADOOPVERSION}\" \\\n      -h \"org.apache.hadoop:hadoop-hdfs:$${HADOOPVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-hdfs-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:druid-kafka-eight:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions:mysql-metadata-storage:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-google-extensions:$${DRUIDVERSION}\" \\\n      -c \"org.apache.druid.extensions.contrib:druid-distinctcount:$${DRUIDVERSION}\"\n\n  curl -L $${MYSQLURL} -o mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 unzip mysql-$${MYSQLVER}.zip \\\n  \u0026\u0026 mv mysql-connector-java-$${MYSQLVER}/mysql-connector-java-$${MYSQLVER}.jar $${DRUIDBASEDIR}/extensions/mysql-metadata-storage\n}\n\n#######################################\n# Downloads the configuration files and gcs-connector-hadoop2 jar file of DataProc \n# from bucket defined by Terraform to the target druid node\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_dataproc_files(){\n  gsutil cp gs://${bucket}/config/core-site.xml  $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/hdfs-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/mapred-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/yarn-site.xml $${DRUIDCONFIGDIR}/druid/_common \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/extensions/druid-google-extensions \\\n    \u0026\u0026 gsutil cp gs://${bucket}/config/gcs-connector-hadoop2* $${DRUIDBASEDIR}/lib\n}\n\n#######################################\n# Create the log directory for logging and change the number of threads in runtime.properties file\n# of historical and middleManager to be able to run on desired machine configuration.\n# It also defines the bucket in which logs must be stored in middleManager/runtime.properties and \n# run the historical service\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction configure_run_druid() {\n  mkdir -p $${DRUIDLOGSDIR}\n  mkdir -p $${DRUIDCONFIGDIR}/druid/router\n  mkdir /var/tmp\n\n  cat \u003e $${DRUIDCONFIGDIR}/druid/router/jvm.config \u003c\u003c EOF\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/tmp/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/var/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\nEOF\n\n\n  cat \u003e $${DRUIDCONFIGDIR}/druid/router/runtime.properties \u003c\u003c EOF\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.tierToBrokerMap={\"_default_tier\":\"druid/broker\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the router proxy http client\ndruid.router.http.numMaxThreads=100\ndruid.server.http.numThreads=100\nEOF\n\ncd $${DRUIDBASEDIR} \n\njava $(cat $${DRUIDCONFIGDIR}/druid/router/jvm.config | xargs) -cp \"$${DRUIDCONFIGDIR}/druid/_common:$${DRUIDCONFIGDIR}/druid/router:lib/*:$${DRUIDBASEDIR}/hadoop-dependencies/hadoop-client/$${HADOOPVERSION}/*\" org.apache.druid.cli.Main server router \u003e $${DRUIDLOGSDIR}/router.log 2\u003e\u00261 \u0026\n\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  install_java\n  get_druid\n  get_dataproc_files\n  configure_run_druid\n  \n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "1",
                            "vars.bucket": "terraform-stage-druid-latest"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "kafka"
            ],
            "outputs": {},
            "resources": {
                "data.template_file.kafka_config": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "dc749f1215387c1329f2fc142c0fe513e5e1b4da6c07d735f7c4f9c12ef5fcdf",
                        "attributes": {
                            "id": "dc749f1215387c1329f2fc142c0fe513e5e1b4da6c07d735f7c4f9c12ef5fcdf",
                            "rendered": "# Downloads Kafka from provided URL, extracts, configures with Zookeeper IP\n# and runs\n\nKAFKA_VER=\"2.11-0.10.2.0\"\nKAFKA_BASEDIR=\"/opt/kafka/kafka_${KAFKA_VER}\"\nKAFKA_CONFIGDIR=\"${KAFKA_BASEDIR}/config\"\nKAFKA_URL=\"https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\n#######################################\n# Downloads kafka packages and extracts on the system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_kafka() {\n  mkdir -p ${KAFKA_BASEDIR} \\\n    \u0026\u0026 curl ${KAFKA_URL} -o /opt/kafka_${KAFKA_VER}.tgz \\\n    \u0026\u0026 tar -xzf /opt/kafka_${KAFKA_VER}.tgz --directory /opt/kafka\n}\n\n#######################################\n# Configures server.properties file with zookeeper cluster ip address provided by Terraform\n# Creates a random broker id between 0-50, starts the service, and logs in /var/log.kafka.log \n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_kafka() {\n  sed -i 's|zookeeper.connect=localhost:2181||g' ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 echo 'zookeeper.connect=10.128.0.41:2181,10.128.0.43:2181,10.128.0.55:2181' \\\n        \u003e\u003e ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 broker=$(shuf -i 0-50 -n 1) \\\n    \u0026\u0026  sed -i \"s/broker.id=0/broker.id=$broker/\" \\\n        ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 ${KAFKA_BASEDIR}/bin/kafka-server-start.sh \\\n        ${KAFKA_CONFIGDIR}/server.properties \u003e /var/log/kafka.log 2\u003e\u00261 \u0026\n}\n\n\n#######################################\n# Removes downloaded Kafka tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction cleanup() {\n sudo rm -rf /opt/kafka/kafka_${KAFKA_VER}.tar.gz\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_kafka\n  conf_run_kafka\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "# Downloads Kafka from provided URL, extracts, configures with Zookeeper IP\n# and runs\n\nKAFKA_VER=\"2.11-0.10.2.0\"\nKAFKA_BASEDIR=\"/opt/kafka/kafka_$${KAFKA_VER}\"\nKAFKA_CONFIGDIR=\"$${KAFKA_BASEDIR}/config\"\nKAFKA_URL=\"https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\n#######################################\n# Downloads kafka packages and extracts on the system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_kafka() {\n  mkdir -p $${KAFKA_BASEDIR} \\\n    \u0026\u0026 curl $${KAFKA_URL} -o /opt/kafka_$${KAFKA_VER}.tgz \\\n    \u0026\u0026 tar -xzf /opt/kafka_$${KAFKA_VER}.tgz --directory /opt/kafka\n}\n\n#######################################\n# Configures server.properties file with zookeeper cluster ip address provided by Terraform\n# Creates a random broker id between 0-50, starts the service, and logs in /var/log.kafka.log \n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_kafka() {\n  sed -i 's|zookeeper.connect=localhost:2181||g' $${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 echo 'zookeeper.connect=${zookeeper0}:2181,${zookeeper1}:2181,${zookeeper2}:2181' \\\n        \u003e\u003e $${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 broker=$(shuf -i 0-50 -n 1) \\\n    \u0026\u0026  sed -i \"s/broker.id=0/broker.id=$broker/\" \\\n        $${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 $${KAFKA_BASEDIR}/bin/kafka-server-start.sh \\\n        $${KAFKA_CONFIGDIR}/server.properties \u003e /var/log/kafka.log 2\u003e\u00261 \u0026\n}\n\n\n#######################################\n# Removes downloaded Kafka tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction cleanup() {\n sudo rm -rf /opt/kafka/kafka_$${KAFKA_VER}.tar.gz\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_kafka\n  conf_run_kafka\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "3",
                            "vars.zookeeper0": "10.128.0.41",
                            "vars.zookeeper1": "10.128.0.43",
                            "vars.zookeeper2": "10.128.0.55"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.kafka.1": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "data.template_file.kafka_config"
                    ],
                    "primary": {
                        "id": "kafka-test-druid-latest-2",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "10",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/kafka-test-druid-latest-2",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "kafka-test-druid-latest-2",
                            "instance_id": "4027497569887571330",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-1",
                            "metadata.%": "1",
                            "metadata.startup-script": "# Downloads Kafka from provided URL, extracts, configures with Zookeeper IP\n# and runs\n\nKAFKA_VER=\"2.11-0.10.2.0\"\nKAFKA_BASEDIR=\"/opt/kafka/kafka_${KAFKA_VER}\"\nKAFKA_CONFIGDIR=\"${KAFKA_BASEDIR}/config\"\nKAFKA_URL=\"https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz\"\n\n#######################################\n# Installs java8 from genuine oracle repository and also updates/upgrades the\n# packages on system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\n#######################################\n# Downloads kafka packages and extracts on the system\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction get_kafka() {\n  mkdir -p ${KAFKA_BASEDIR} \\\n    \u0026\u0026 curl ${KAFKA_URL} -o /opt/kafka_${KAFKA_VER}.tgz \\\n    \u0026\u0026 tar -xzf /opt/kafka_${KAFKA_VER}.tgz --directory /opt/kafka\n}\n\n#######################################\n# Configures server.properties file with zookeeper cluster ip address provided by Terraform\n# Creates a random broker id between 0-50, starts the service, and logs in /var/log.kafka.log \n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_kafka() {\n  sed -i 's|zookeeper.connect=localhost:2181||g' ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 echo 'zookeeper.connect=10.128.0.41:2181,10.128.0.43:2181,10.128.0.55:2181' \\\n        \u003e\u003e ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 broker=$(shuf -i 0-50 -n 1) \\\n    \u0026\u0026  sed -i \"s/broker.id=0/broker.id=$broker/\" \\\n        ${KAFKA_CONFIGDIR}/server.properties \\\n    \u0026\u0026 ${KAFKA_BASEDIR}/bin/kafka-server-start.sh \\\n        ${KAFKA_CONFIGDIR}/server.properties \u003e /var/log/kafka.log 2\u003e\u00261 \u0026\n}\n\n\n#######################################\n# Removes downloaded Kafka tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction cleanup() {\n sudo rm -rf /opt/kafka/kafka_${KAFKA_VER}.tar.gz\n}\n\nmain(){\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_kafka\n  conf_run_kafka\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "metadata_fingerprint": "fNyDEJiR5E4=",
                            "min_cpu_platform": "",
                            "name": "kafka-test-druid-latest-2",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.239.250.39",
                            "network_interface.0.access_config.0.nat_ip": "35.239.250.39",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.44",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.44",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/kafka-test-druid-latest-2",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "3",
                            "service_account.0.scopes.1632638332": "https://www.googleapis.com/auth/devstorage.read_only",
                            "service_account.0.scopes.2428168921": "https://www.googleapis.com/auth/userinfo.email",
                            "service_account.0.scopes.2862113455": "https://www.googleapis.com/auth/compute.readonly",
                            "tags.#": "2",
                            "tags.1517147638": "server",
                            "tags.1539077399": "kafka",
                            "tags_fingerprint": "VuWhCT06SUQ=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        },
        {
            "path": [
                "root",
                "zookeeper"
            ],
            "outputs": {
                "zookeeper_ip_list": {
                    "sensitive": false,
                    "type": "list",
                    "value": [
                        "10.128.0.41",
                        "10.128.0.43",
                        "10.128.0.55"
                    ]
                }
            },
            "resources": {
                "data.template_file.zookeeper_config": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "3d4f30eae75c6b1bedf7bd90ed1c6cc54a6758a864645ad82b33554bf430fdf7",
                        "attributes": {
                            "id": "3d4f30eae75c6b1bedf7bd90ed1c6cc54a6758a864645ad82b33554bf430fdf7",
                            "rendered": "# Downloads Zookeeper from provided URL, extracts, configures it with cluster name address,\n# and runs\n\nZOOKEEPER_VER=3.4.10\nZOOKEEPER_BASEDIR=\"/opt/zookeeper/zookeeper-${ZOOKEEPER_VER}\"\nZOOKEEPER_CONFIGDIR=\"${ZOOKEEPER_BASEDIR}/conf\"\nZOOKEEPER_URL=\"http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz\"\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\nfunction get_zookeeper() {\n  sudo mkdir -p ${ZOOKEEPER_BASEDIR}/data/zookeeper \\\n    \u0026\u0026 sudo curl ${ZOOKEEPER_URL} -o /opt/zookeeper-${ZOOKEEPER_VER}.tar.gz \\\n    \u0026\u0026 sudo tar -xzf /opt/zookeeper-3.4.10.tar.gz --directory /opt/zookeeper\n}\n\n#######################################\n# Configures zoo.cfg file with zookeeper cluster ip address provided by Terraform\n# Creates ${ZOOKEEPER_BASEDIR}/data/zookeeper/myid with the provided by Terraform\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_zookeeper() {\n  sudo cp ${ZOOKEEPER_CONFIGDIR}/zoo_sample.cfg ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 sudo bash -c \"cat /usr/share/zookeeper.id \u003e ${ZOOKEEPER_BASEDIR}/data/zookeeper/myid\" \\\n    \u0026\u0026 sudo sed -i \"s|/tmp|${ZOOKEEPER_BASEDIR}/data|g\" ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.1=zoo-test-druid-latest-1:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.2=zoo-test-druid-latest-2:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.3=zoo-test-druid-latest-3:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 ${ZOOKEEPER_BASEDIR}/bin/./zkServer.sh start\n}\n\n#######################################\n# Removes downloaded Zookeeper tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction cleanup() {\n sudo rm -rf /opt/zookeeper-${ZOOKEEPER_VER}.tar.gz\n}\n\nmain() {\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_zookeeper\n  conf_run_zookeeper\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "template": "# Downloads Zookeeper from provided URL, extracts, configures it with cluster name address,\n# and runs\n\nZOOKEEPER_VER=3.4.10\nZOOKEEPER_BASEDIR=\"/opt/zookeeper/zookeeper-$${ZOOKEEPER_VER}\"\nZOOKEEPER_CONFIGDIR=\"$${ZOOKEEPER_BASEDIR}/conf\"\nZOOKEEPER_URL=\"http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz\"\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\nfunction get_zookeeper() {\n  sudo mkdir -p $${ZOOKEEPER_BASEDIR}/data/zookeeper \\\n    \u0026\u0026 sudo curl $${ZOOKEEPER_URL} -o /opt/zookeeper-$${ZOOKEEPER_VER}.tar.gz \\\n    \u0026\u0026 sudo tar -xzf /opt/zookeeper-3.4.10.tar.gz --directory /opt/zookeeper\n}\n\n#######################################\n# Configures zoo.cfg file with zookeeper cluster ip address provided by Terraform\n# Creates $${ZOOKEEPER_BASEDIR}/data/zookeeper/myid with the provided by Terraform\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_zookeeper() {\n  sudo cp $${ZOOKEEPER_CONFIGDIR}/zoo_sample.cfg $${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 sudo bash -c \"cat /usr/share/zookeeper.id \u003e $${ZOOKEEPER_BASEDIR}/data/zookeeper/myid\" \\\n    \u0026\u0026 sudo sed -i \"s|/tmp|$${ZOOKEEPER_BASEDIR}/data|g\" $${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.1=${zookeeper0}:2888:3888\" \u003e\u003e $${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.2=${zookeeper1}:2888:3888\" \u003e\u003e $${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.3=${zookeeper2}:2888:3888\" \u003e\u003e $${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 $${ZOOKEEPER_BASEDIR}/bin/./zkServer.sh start\n}\n\n#######################################\n# Removes downloaded Zookeeper tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction cleanup() {\n sudo rm -rf /opt/zookeeper-$${ZOOKEEPER_VER}.tar.gz\n}\n\nmain() {\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_zookeeper\n  conf_run_zookeeper\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "vars.%": "3",
                            "vars.zookeeper0": "zoo-test-druid-latest-1",
                            "vars.zookeeper1": "zoo-test-druid-latest-2",
                            "vars.zookeeper2": "zoo-test-druid-latest-3"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "google_compute_instance.zookeeper.0": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "google_storage_bucket_object.zoo-init-script"
                    ],
                    "primary": {
                        "id": "zoo-test-druid-latest-1",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "10",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/zoo-test-druid-latest-1",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "zoo-test-druid-latest-1",
                            "instance_id": "5662077473234734504",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-1",
                            "metadata.%": "1",
                            "metadata.startup-script": "echo 1 \u003e /usr/share/zookeeper.id; gsutil cp gs://terraform-stage-druid-latest/zookeeper_script.sh /opt; chmod +x /opt/zookeeper_script.sh; /opt/./zookeeper_script.sh",
                            "metadata_fingerprint": "usR_dfEEDDA=",
                            "min_cpu_platform": "",
                            "name": "zoo-test-druid-latest-1",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "104.197.69.109",
                            "network_interface.0.access_config.0.nat_ip": "104.197.69.109",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.41",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.41",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/zoo-test-druid-latest-1",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "3",
                            "service_account.0.scopes.1632638332": "https://www.googleapis.com/auth/devstorage.read_only",
                            "service_account.0.scopes.2428168921": "https://www.googleapis.com/auth/userinfo.email",
                            "service_account.0.scopes.2862113455": "https://www.googleapis.com/auth/compute.readonly",
                            "tags.#": "3",
                            "tags.1517147638": "server",
                            "tags.2305736249": "myid-1",
                            "tags.987247817": "zookeeper",
                            "tags_fingerprint": "vpTvBTX4nmo=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_compute_instance.zookeeper.1": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "google_storage_bucket_object.zoo-init-script"
                    ],
                    "primary": {
                        "id": "zoo-test-druid-latest-2",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "10",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/zoo-test-druid-latest-2",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "zoo-test-druid-latest-2",
                            "instance_id": "8820199245612436953",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-1",
                            "metadata.%": "1",
                            "metadata.startup-script": "echo 2 \u003e /usr/share/zookeeper.id; gsutil cp gs://terraform-stage-druid-latest/zookeeper_script.sh /opt; chmod +x /opt/zookeeper_script.sh; /opt/./zookeeper_script.sh",
                            "metadata_fingerprint": "hPqB4yfIwL0=",
                            "min_cpu_platform": "",
                            "name": "zoo-test-druid-latest-2",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.239.21.177",
                            "network_interface.0.access_config.0.nat_ip": "35.239.21.177",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.43",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.43",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/zoo-test-druid-latest-2",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "3",
                            "service_account.0.scopes.1632638332": "https://www.googleapis.com/auth/devstorage.read_only",
                            "service_account.0.scopes.2428168921": "https://www.googleapis.com/auth/userinfo.email",
                            "service_account.0.scopes.2862113455": "https://www.googleapis.com/auth/compute.readonly",
                            "tags.#": "3",
                            "tags.1517147638": "server",
                            "tags.275246979": "myid-2",
                            "tags.987247817": "zookeeper",
                            "tags_fingerprint": "UJpEV8td4dc=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_compute_instance.zookeeper.2": {
                    "type": "google_compute_instance",
                    "depends_on": [
                        "google_storage_bucket_object.zoo-init-script"
                    ],
                    "primary": {
                        "id": "zoo-test-druid-latest-3",
                        "attributes": {
                            "attached_disk.#": "0",
                            "boot_disk.#": "1",
                            "boot_disk.0.auto_delete": "true",
                            "boot_disk.0.device_name": "persistent-disk-0",
                            "boot_disk.0.disk_encryption_key_raw": "",
                            "boot_disk.0.disk_encryption_key_sha256": "",
                            "boot_disk.0.initialize_params.#": "1",
                            "boot_disk.0.initialize_params.0.image": "https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/ubuntu-1604-xenial-v20190122a",
                            "boot_disk.0.initialize_params.0.size": "10",
                            "boot_disk.0.initialize_params.0.type": "pd-standard",
                            "boot_disk.0.source": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/disks/zoo-test-druid-latest-3",
                            "can_ip_forward": "false",
                            "cpu_platform": "Intel Sandy Bridge",
                            "create_timeout": "4",
                            "deletion_protection": "false",
                            "guest_accelerator.#": "0",
                            "id": "zoo-test-druid-latest-3",
                            "instance_id": "1973810879441259972",
                            "label_fingerprint": "42WmSpB8rSM=",
                            "labels.%": "0",
                            "machine_type": "n1-standard-1",
                            "metadata.%": "1",
                            "metadata.startup-script": "echo 3 \u003e /usr/share/zookeeper.id; gsutil cp gs://terraform-stage-druid-latest/zookeeper_script.sh /opt; chmod +x /opt/zookeeper_script.sh; /opt/./zookeeper_script.sh",
                            "metadata_fingerprint": "uOXINvOWkAY=",
                            "min_cpu_platform": "",
                            "name": "zoo-test-druid-latest-3",
                            "network_interface.#": "1",
                            "network_interface.0.access_config.#": "1",
                            "network_interface.0.access_config.0.assigned_nat_ip": "35.232.136.3",
                            "network_interface.0.access_config.0.nat_ip": "35.232.136.3",
                            "network_interface.0.access_config.0.network_tier": "PREMIUM",
                            "network_interface.0.access_config.0.public_ptr_domain_name": "",
                            "network_interface.0.address": "10.128.0.55",
                            "network_interface.0.alias_ip_range.#": "0",
                            "network_interface.0.name": "nic0",
                            "network_interface.0.network": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/global/networks/default",
                            "network_interface.0.network_ip": "10.128.0.55",
                            "network_interface.0.subnetwork": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/regions/us-central1/subnetworks/default",
                            "network_interface.0.subnetwork_project": "pso-druid-presto-migration",
                            "project": "pso-druid-presto-migration",
                            "scheduling.#": "1",
                            "scheduling.0.automatic_restart": "false",
                            "scheduling.0.on_host_maintenance": "MIGRATE",
                            "scheduling.0.preemptible": "false",
                            "scratch_disk.#": "0",
                            "self_link": "https://www.googleapis.com/compute/v1/projects/pso-druid-presto-migration/zones/us-central1-a/instances/zoo-test-druid-latest-3",
                            "service_account.#": "1",
                            "service_account.0.email": "1026842386029-compute@developer.gserviceaccount.com",
                            "service_account.0.scopes.#": "3",
                            "service_account.0.scopes.1632638332": "https://www.googleapis.com/auth/devstorage.read_only",
                            "service_account.0.scopes.2428168921": "https://www.googleapis.com/auth/userinfo.email",
                            "service_account.0.scopes.2862113455": "https://www.googleapis.com/auth/compute.readonly",
                            "tags.#": "3",
                            "tags.1517147638": "server",
                            "tags.1734401813": "myid-3",
                            "tags.987247817": "zookeeper",
                            "tags_fingerprint": "QF-o2Oo-PyM=",
                            "zone": "us-central1-a"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 360000000000,
                                "delete": 360000000000,
                                "update": 360000000000
                            },
                            "schema_version": "6"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                },
                "google_storage_bucket_object.zoo-init-script": {
                    "type": "google_storage_bucket_object",
                    "depends_on": [
                        "data.template_file.zookeeper_config"
                    ],
                    "primary": {
                        "id": "terraform-stage-druid-latest-zookeeper_script.sh",
                        "attributes": {
                            "bucket": "terraform-stage-druid-latest",
                            "cache_control": "",
                            "content": "# Downloads Zookeeper from provided URL, extracts, configures it with cluster name address,\n# and runs\n\nZOOKEEPER_VER=3.4.10\nZOOKEEPER_BASEDIR=\"/opt/zookeeper/zookeeper-${ZOOKEEPER_VER}\"\nZOOKEEPER_CONFIGDIR=\"${ZOOKEEPER_BASEDIR}/conf\"\nZOOKEEPER_URL=\"http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz\"\n\nfunction get_java() {\n  apt-get -y update \u0026\u0026 apt-get -y upgrade \\\n  \u0026\u0026 apt-get install -y default-jdk\n}\n\nfunction get_zookeeper() {\n  sudo mkdir -p ${ZOOKEEPER_BASEDIR}/data/zookeeper \\\n    \u0026\u0026 sudo curl ${ZOOKEEPER_URL} -o /opt/zookeeper-${ZOOKEEPER_VER}.tar.gz \\\n    \u0026\u0026 sudo tar -xzf /opt/zookeeper-3.4.10.tar.gz --directory /opt/zookeeper\n}\n\n#######################################\n# Configures zoo.cfg file with zookeeper cluster ip address provided by Terraform\n# Creates ${ZOOKEEPER_BASEDIR}/data/zookeeper/myid with the provided by Terraform\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\nfunction conf_run_zookeeper() {\n  sudo cp ${ZOOKEEPER_CONFIGDIR}/zoo_sample.cfg ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 sudo bash -c \"cat /usr/share/zookeeper.id \u003e ${ZOOKEEPER_BASEDIR}/data/zookeeper/myid\" \\\n    \u0026\u0026 sudo sed -i \"s|/tmp|${ZOOKEEPER_BASEDIR}/data|g\" ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.1=zoo-test-druid-latest-1:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.2=zoo-test-druid-latest-2:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 echo \"server.3=zoo-test-druid-latest-3:2888:3888\" \u003e\u003e ${ZOOKEEPER_CONFIGDIR}/zoo.cfg \\\n    \u0026\u0026 ${ZOOKEEPER_BASEDIR}/bin/./zkServer.sh start\n}\n\n#######################################\n# Removes downloaded Zookeeper tar file\n# Globals:\n#   None\n# Arguments:\n#   None\n# Returns:\n#   None\n#######################################\n\n\nfunction cleanup() {\n sudo rm -rf /opt/zookeeper-${ZOOKEEPER_VER}.tar.gz\n}\n\nmain() {\nif [[ ! -f ~/startup-flag ]]; then #startup-flag file used as a flag to run only once as initialization script\n  get_java\n  get_zookeeper\n  conf_run_zookeeper\n  cleanup\n\n  touch ~/startup-flag\nfi\n}\n\nmain\n",
                            "content_disposition": "",
                            "content_encoding": "",
                            "content_language": "",
                            "content_type": "text/plain; charset=utf-8",
                            "crc32c": "drgQBw==",
                            "detect_md5hash": "rVn85pjDQnRIBDX0GTxx/g==",
                            "id": "terraform-stage-druid-latest-zookeeper_script.sh",
                            "md5hash": "rVn85pjDQnRIBDX0GTxx/g==",
                            "name": "zookeeper_script.sh",
                            "storage_class": "STANDARD"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.google"
                }
            },
            "depends_on": []
        }
    ]
}
