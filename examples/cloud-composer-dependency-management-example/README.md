# Composer Dependency Management

##### This repo contains an example Cloud Composer workflow that provides a solution for managing varying dependencies between Composer Airflow DAGs, specifically focusing on DAGs with yearly, monthly, and weekly frequencies acting as parent jobs. The solution ensures that child DAGs are triggered appropriately based on their parent's schedule.

The goal of this example is to provide a common pattern to automatically trigger and implement the composer dependency management. The primary challenge addressed is the need to handle complex dependencies between DAGs with different frequencies. The solution leverages Airflow's dependency management capabilities to create a hierarchical relationship between the parent and child DAGs.

### Workflow Overview

***

![Alt text](../img/composer_mgmt_usecase.png "Workflow Overview")

The workflow involves the following steps:

1. **Create Parent DAGs**:
    - Create separate DAGs for each parent job (yearly, monthly, and weekly).
    - Define the schedule for each parent DAG accordingly.

2. **Define Child DAGs**:
    - Create child DAGs for each task that needs to be executed based on the parent's schedule.

3. **Set Dependencies**:
    - Use the `ExternalTaskSensor` argument to establish the dependency between the child DAG and its immediate parent DAG.

4. **Trigger Child DAGs**:
    - Utilize Airflow's `TriggerDagRunOperator` to trigger child DAGs when the parent DAG completes.
    - Configure the `wait_for_downstream` parameter to specify the conditions under which the child DAG should be triggered.

5. **Handle Data Lineage**:
    - Ensure that the child DAGs have access to the necessary data generated by the parent DAG.
    - Consider using Airflow's XComs or a central data store for data sharing.

### Benefits
- Improved DAG organization and maintainability.
- Simplified dependency management.
- Reliable execution of child DAGs based on parent schedules.
- Reduced risk of data inconsistencies.
- Scalable approach for managing complex DAG dependencies.

## Use Case: The Symphony of Data Orchestration
In the bustling city of San Francisco, a dynamic e-commerce company named "Symphony Goods" was on a mission to revolutionize the online shopping experience. At the heart of their success was a robust data infrastructure that seamlessly managed and processed vast amounts of information.

### Symphony Goods Data Workflows
Symphony Goods relied on a sophisticated data orchestration system powered by Apache Airflow to automate and streamline their data workflows. This system consisted of a series of interconnected data pipelines, each designed to perform specific tasks and produce valuable insights.

#### Yearly Refresh: Company Calendar
Once a year, Symphony Goods executed a critical process known as ["Company_cal_refresh"](company_cal_refresh.py). This workflow ensured that the company's internal calendars were synchronized across all departments and systems. It involved extracting data from various sources, such as employee schedules, project timelines, and public holidays, and consolidating it into a centralized repository. The updated calendar served as a single source of truth, enabling efficient planning, resource allocation, and communication within the organization.

#### Monthly Refresh: Product Catalog
Every month, Symphony Goods performed a "Product_catalog_refresh" workflow to keep its product catalog up-to-date. This process involved ingesting data from multiple channels, including supplier feeds, internal databases, and customer feedback. The workflow validated, transformed, and enriched the product information, ensuring that customers had access to accurate and comprehensive product details.

#### Weekly Summary Report
Symphony Goods generated a "Weekly_summary_report" every week to monitor key performance indicators (KPIs) and track business growth. The workflow aggregated data from various sources, such as sales figures, customer engagement metrics, and website traffic analytics. It then presented the data in visually appealing dashboards and reports, enabling stakeholders to make informed decisions.

#### Daily Refresh: Product Inventory
To ensure optimal inventory management, Symphony Goods ran a ["Product_inventory_refresh"](product_catalog_refresh.py) workflow on a daily basis. This workflow extracted inventory data from warehouses, distribution centers, and point-of-sale systems. It calculated available stock levels, identified potential stockouts, and provided recommendations for replenishment. The workflow ensured that Symphony Goods could fulfill customer orders promptly and maintain high levels of customer satisfaction.

The symphony of data orchestration at Symphony Goods was a testament to the power of automation and integration. By leveraging Apache Airflow, the company was able to streamline its data operations, improve data quality, and gain valuable insights to drive business growth. As Symphony Goods continued to scale its operations, the data orchestration system served as the backbone, ensuring that data was always available, accurate, and actionable.

### Workflow Frequencies
1. **Yearly**: [Company_cal_refresh](company_cal_refresh.py) 
2. **Monthly**: [Product_catalog_refresh](product_catalog_refresh.py)
3. **Weekly**: [Weekly_summary_report](weekly_summary_report.py)
4. **Daily**: [Product_inventory_refresh](product_inventory_refresh.py)

## Use-case Lineage: Summary of Lineage and Dependencies
The provided context describes the data orchestration system used by Symphony Goods, an e-commerce company in San Francisco. The system is powered by Apache Airflow and consists of four main workflows:

1. **Yearly: Company_cal_refresh**
    - Synchronizes internal calendars across all departments and systems, ensuring efficient planning and resource allocation.
    - Depends on data from employee schedules, project timelines, and public holidays.

2. **Monthly: Product_catalog_refresh**
    - Keeps the product catalog up-to-date by ingesting data from multiple channels and validating, transforming, and enriching it.
    - Depends on data from supplier feeds, internal databases, and customer feedback.

3. **Weekly: Weekly_summary_report**
    - Generates weekly summary reports to monitor key performance indicators (KPIs) and track business growth.
    - Depends on data from sales figures, customer engagement metrics, and website traffic analytics.

4. **Daily: Product_inventory_refresh**
    - Ensures optimal inventory management by extracting inventory data from various sources and calculating available stock levels.
    - Depends on data from warehouses, distribution centers, and point-of-sale systems.

The symphony of data orchestration at Symphony Goods is a testament to the power of automation and integration. By leveraging Apache Airflow, the company was able to streamline its data operations, improve data quality, and gain valuable insights to drive business growth.
